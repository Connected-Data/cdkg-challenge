1
00:00:08,360 --> 00:00:09,800
Hi, everyone, and
I'm very happy

2
00:00:09,800 --> 00:00:10,855
to be here today.

3
00:00:10,855 --> 00:00:12,295
Hope you are enjoying so far

4
00:00:12,295 --> 00:00:13,435
from the different sessions.

5
00:00:14,135 --> 00:00:15,835
This session will
have two parts

6
00:00:15,975 --> 00:00:17,835
jointly given by Szymon Klarman

7
00:00:17,895 --> 00:00:20,155
and myself. My name
is David Amzalag,

8
00:00:20,540 --> 00:00:22,060
and I'm the chief
product officer

9
00:00:22,060 --> 00:00:23,280
of Black Swan Technologies.

10
00:00:23,900 --> 00:00:25,580
Black Swan is
a growing startup

11
00:00:25,580 --> 00:00:27,120
that specialize in data.

12
00:00:27,420 --> 00:00:29,260
We have presences
in six different

13
00:00:29,260 --> 00:00:30,885
countries, more
than two hundred

14
00:00:30,885 --> 00:00:31,925
and fifty employees,

15
00:00:31,925 --> 00:00:33,305
and different type
of customers.

16
00:00:34,485 --> 00:00:35,545
Why different types?

17
00:00:35,685 --> 00:00:38,510
Because every company,
enterprises,

18
00:00:38,650 --> 00:00:40,730
for instance, is having a very

19
00:00:40,730 --> 00:00:42,430
large number of data sources.

20
00:00:43,290 --> 00:00:45,370
Sometimes hundreds
of different

21
00:00:45,370 --> 00:00:47,550
source data sources
and sometimes

22
00:00:47,755 --> 00:00:49,355
thousands of different data

23
00:00:49,355 --> 00:00:51,135
sources that the organization

24
00:00:51,355 --> 00:00:53,355
is relying on and
using him for

25
00:00:53,355 --> 00:00:54,495
all type of usages.

26
00:00:54,940 --> 00:00:56,720
Obviously, regardless
the industry,

27
00:00:56,940 --> 00:00:58,960
those enterprises
are belong to,

28
00:00:59,180 --> 00:01:01,360
health care, finance,
insurances,

29
00:01:01,660 --> 00:01:02,640
retail, etcetera.

30
00:01:03,675 --> 00:01:05,755
What is common
with all of them

31
00:01:05,755 --> 00:01:08,875
is the real and
big concern they

32
00:01:08,875 --> 00:01:11,295
have on how to
effectively manage

33
00:01:11,355 --> 00:01:12,815
all of those data sources,

34
00:01:13,050 --> 00:01:14,510
keeping them synchronized,

35
00:01:14,810 --> 00:01:16,490
accessing them in real time,

36
00:01:16,490 --> 00:01:17,930
keeping the infrastructure not

37
00:01:17,930 --> 00:01:19,950
so expensive, and
very important,

38
00:01:20,170 --> 00:01:22,270
how to monetize
valuable insight

39
00:01:22,755 --> 00:01:24,115
from all of them.

40
00:01:24,115 --> 00:01:25,815
Sounds like a big
problem. No?

41
00:01:26,915 --> 00:01:28,355
Let's make this problem even

42
00:01:28,355 --> 00:01:29,095
more realistic.

43
00:01:29,395 --> 00:01:31,315
Almost all data
sources are feed

44
00:01:31,315 --> 00:01:32,375
by different applications.

45
00:01:33,220 --> 00:01:35,640
Salesforce Salesforce
and other CRMs,

46
00:01:35,940 --> 00:01:37,080
NetSuite, AWS,

47
00:01:37,780 --> 00:01:39,480
Facebook Ministry
of Interior,

48
00:01:39,620 --> 00:01:42,345
etcetera. Those probably are

49
00:01:42,345 --> 00:01:44,285
geographically
spread in addition.

50
00:01:45,145 --> 00:01:46,985
Now what we are doing in many

51
00:01:46,985 --> 00:01:49,430
of the cases is to
build a data lake,

52
00:01:49,430 --> 00:01:51,510
one centralized data lake from

53
00:01:51,510 --> 00:01:54,390
all data sources
that this means

54
00:01:54,390 --> 00:01:55,750
that the data
will be stored at

55
00:01:55,750 --> 00:01:56,650
the same place.

56
00:01:57,065 --> 00:01:59,545
No doubt that this might help

57
00:01:59,545 --> 00:02:01,165
operationally. However,

58
00:02:01,225 --> 00:02:02,845
this is not so
cheap solution,

59
00:02:03,145 --> 00:02:04,825
and data is not
kept synchronized

60
00:02:04,825 --> 00:02:06,045
in many of the cases.

61
00:02:06,590 --> 00:02:08,430
Moreover, once
the data is kept

62
00:02:08,430 --> 00:02:09,570
in a single place,

63
00:02:10,030 --> 00:02:11,490
different personas
of the organization

64
00:02:12,190 --> 00:02:14,610
might have different
usages of the data.

65
00:02:14,815 --> 00:02:16,575
Different queries
might be asked,

66
00:02:16,575 --> 00:02:18,115
and data from
different sources

67
00:02:18,575 --> 00:02:20,435
must be consistent
and aligned.

68
00:02:20,815 --> 00:02:22,515
Actually, a very big problem.

69
00:02:23,620 --> 00:02:25,320
Black Swan created technology

70
00:02:25,540 --> 00:02:27,220
so no data leak is needed,

71
00:02:27,220 --> 00:02:28,980
and access to data source is

72
00:02:28,980 --> 00:02:30,905
done directly to
where they are.

73
00:02:31,145 --> 00:02:33,005
Geography here is
not so important.

74
00:02:33,545 --> 00:02:35,405
Interface is being
done throughout

75
00:02:35,465 --> 00:02:37,965
the no code type of
user experience,

76
00:02:38,560 --> 00:02:40,960
So people from all type of

77
00:02:40,960 --> 00:02:43,140
organizations,
meaning accounting,

78
00:02:43,520 --> 00:02:44,980
hospital data scientists,

79
00:02:45,120 --> 00:02:46,660
investors in bank, etcetera,

80
00:02:46,955 --> 00:02:48,815
will be able to
create applications

81
00:02:48,875 --> 00:02:51,115
and queries by
themselves actually

82
00:02:51,115 --> 00:02:53,375
very easily and
really to generate

83
00:02:53,515 --> 00:02:55,900
insight from the data
with without

84
00:02:55,960 --> 00:02:57,260
being, let's say,

85
00:02:57,800 --> 00:02:59,340
very good software people.

86
00:03:00,040 --> 00:03:01,740
The insight is being generated

87
00:03:01,800 --> 00:03:04,205
continuously
throughout a large

88
00:03:04,205 --> 00:03:05,805
infrastructure of knowledge

89
00:03:05,805 --> 00:03:07,185
graphs and its
generalizations.

90
00:03:08,605 --> 00:03:09,805
Thank you very much for your

91
00:03:09,805 --> 00:03:11,425
time and handing
over to Shiman

92
00:03:11,485 --> 00:03:13,300
to speak about
our interpretation

93
00:03:13,600 --> 00:03:14,800
of knowledge graph.

94
00:03:14,800 --> 00:03:16,580
We call it
the knowledge mesh.

95
00:03:17,600 --> 00:03:19,540
Hi. My name is
Shimon Klarman,

96
00:03:19,600 --> 00:03:20,800
and I'm a knowledge architect

97
00:03:20,800 --> 00:03:21,860
at Dexcom Technologies.

98
00:03:22,945 --> 00:03:24,725
As David already explained,

99
00:03:25,585 --> 00:03:27,345
the main challenge
we are trying

100
00:03:27,345 --> 00:03:30,600
to address is how
to facilitate

101
00:03:31,620 --> 00:03:34,840
utilization of
data within large

102
00:03:35,220 --> 00:03:37,895
data intensive
organizations who

103
00:03:37,895 --> 00:03:40,015
commonly own enormous amounts

104
00:03:40,015 --> 00:03:40,995
of data assets,

105
00:03:41,215 --> 00:03:44,515
and yet they struggle to get

106
00:03:44,790 --> 00:03:46,970
the actual insights
and knowledge

107
00:03:47,110 --> 00:03:48,330
out of those assets.

108
00:03:49,270 --> 00:03:50,490
So a prototypical,

109
00:03:52,150 --> 00:03:53,745
starting point for those kind

110
00:03:53,745 --> 00:03:55,345
of scenarios could
be a use case

111
00:03:55,345 --> 00:03:56,705
we could call a three hundred

112
00:03:56,705 --> 00:03:58,485
sixty degree entity view.

113
00:03:59,425 --> 00:04:01,605
So something, that in practice

114
00:04:01,665 --> 00:04:03,820
would be known as
single client view,

115
00:04:03,820 --> 00:04:05,520
know your customer,
compliance,

116
00:04:06,300 --> 00:04:09,020
a scenario where we want to

117
00:04:09,020 --> 00:04:11,020
gather information
about a certain,

118
00:04:12,265 --> 00:04:14,345
real world entity
or collection

119
00:04:14,345 --> 00:04:15,245
of such entities,

120
00:04:16,025 --> 00:04:18,025
where this information
is actually

121
00:04:18,025 --> 00:04:20,185
spread across multiple data

122
00:04:20,185 --> 00:04:21,325
management systems.

123
00:04:22,850 --> 00:04:24,130
It's well known that in this

124
00:04:24,130 --> 00:04:25,110
kind of scenarios,

125
00:04:25,170 --> 00:04:28,370
organization struggle
with a number of,

126
00:04:28,690 --> 00:04:30,895
data challenges such as data

127
00:04:30,895 --> 00:04:32,355
discoverability issues.

128
00:04:33,375 --> 00:04:35,055
Sources are often based in

129
00:04:35,055 --> 00:04:37,155
multiple even multi
cloud environments.

130
00:04:38,310 --> 00:04:38,950
Because of that,

131
00:04:38,950 --> 00:04:40,630
there's lack of uniform

132
00:04:40,630 --> 00:04:41,850
accessibility rules.

133
00:04:42,230 --> 00:04:43,590
There's a heterogeneity of

134
00:04:43,590 --> 00:04:45,130
database models and schemas,

135
00:04:45,525 --> 00:04:47,065
lack of
semantic interoperability

136
00:04:47,605 --> 00:04:49,045
across those schemas in which

137
00:04:49,045 --> 00:04:50,105
data is expressed.

138
00:04:50,965 --> 00:04:52,725
There's a number
of data quality

139
00:04:52,725 --> 00:04:54,265
issues such as data
incompleteness,

140
00:04:54,680 --> 00:04:57,180
granularity, or
data normalization

141
00:04:57,480 --> 00:05:01,100
issues. Now one,

142
00:05:01,640 --> 00:05:03,545
natural way of handling this

143
00:05:03,545 --> 00:05:04,825
kind of three hundred sixty

144
00:05:04,825 --> 00:05:07,545
degree entity
view use cases is

145
00:05:07,545 --> 00:05:09,465
by employing
enterprise knowledge

146
00:05:09,465 --> 00:05:12,365
graphs. I guess within
this audience,

147
00:05:12,425 --> 00:05:12,825
this is,

148
00:05:14,560 --> 00:05:16,660
this is actually a well
known solution.

149
00:05:17,600 --> 00:05:19,120
We know that we would like to

150
00:05:19,120 --> 00:05:20,820
get the clean,
the duplicated,

151
00:05:21,360 --> 00:05:22,660
resolved, reconciled,

152
00:05:22,960 --> 00:05:25,925
up to date view of
all the entity data.

153
00:05:26,865 --> 00:05:29,045
This kind of entity
and relationship

154
00:05:29,265 --> 00:05:33,730
centric, view is arguably much

155
00:05:33,730 --> 00:05:35,410
closer to the real world or at

156
00:05:35,410 --> 00:05:38,210
least the way we think about

157
00:05:38,210 --> 00:05:39,190
the real world.

158
00:05:41,355 --> 00:05:43,695
It presents all
the information

159
00:05:44,075 --> 00:05:47,195
integrated as it's
a very powerful

160
00:05:47,195 --> 00:05:49,450
data integration
framework under

161
00:05:49,450 --> 00:05:52,110
a single consistent
data model.

162
00:05:54,410 --> 00:05:56,590
Further, this
being a a graph,

163
00:05:57,825 --> 00:05:59,365
also a semantic graph,

164
00:05:59,425 --> 00:06:01,205
which we can explain
a bit later,

165
00:06:01,665 --> 00:06:03,845
means that we can
apply a range

166
00:06:04,385 --> 00:06:08,400
of analytical
techniques on top

167
00:06:08,400 --> 00:06:10,340
of such structure
to get insights,

168
00:06:10,400 --> 00:06:12,340
things like semantic inference

169
00:06:12,960 --> 00:06:14,740
or machine learning
on graphs.

170
00:06:15,345 --> 00:06:17,445
So altogether,
if we add it up,

171
00:06:18,625 --> 00:06:21,285
offers us possibility
to get a reliable

172
00:06:21,505 --> 00:06:24,120
knowledge, something
we can trust,

173
00:06:24,580 --> 00:06:26,100
something that gives us a lot

174
00:06:26,100 --> 00:06:28,420
of contextual information out

175
00:06:28,420 --> 00:06:31,400
of just a plain
raw data source.

176
00:06:33,255 --> 00:06:35,095
Of course, we know where we'd

177
00:06:35,095 --> 00:06:37,515
like to get as the ideal,

178
00:06:38,535 --> 00:06:39,595
end of the journey,

179
00:06:39,735 --> 00:06:42,540
but how to get there
is a challenge

180
00:06:42,540 --> 00:06:43,600
in its own right.

181
00:06:43,900 --> 00:06:45,840
And knowledge
graph construction

182
00:06:45,980 --> 00:06:48,940
process, according to some

183
00:06:48,940 --> 00:06:51,185
textbook recipes, could be,

184
00:06:51,505 --> 00:06:52,565
under some simplification

185
00:06:53,185 --> 00:06:54,885
presented as this kind of,

186
00:06:55,345 --> 00:06:58,165
ETL pipeline going
from the sources

187
00:06:59,070 --> 00:07:01,390
to the final
knowledge graph by

188
00:07:01,390 --> 00:07:02,850
the process of extracting,

189
00:07:03,310 --> 00:07:05,570
transforming, and
loading the data,

190
00:07:06,595 --> 00:07:08,295
most likely to
a graph database.

191
00:07:10,515 --> 00:07:12,275
The extraction process would

192
00:07:12,275 --> 00:07:15,910
most commonly be
achieved using type,

193
00:07:17,010 --> 00:07:19,490
sort of extractors
depending on

194
00:07:19,490 --> 00:07:21,730
the specific data sources.

195
00:07:21,730 --> 00:07:22,690
So for instance,

196
00:07:22,690 --> 00:07:24,870
SQL extractors for
structured databases,

197
00:07:25,155 --> 00:07:27,495
scrapers for, HTML websites,

198
00:07:28,195 --> 00:07:30,915
NLP algorithms for processing

199
00:07:30,915 --> 00:07:31,735
text documents.

200
00:07:32,440 --> 00:07:34,140
The data extracted
from extractor

201
00:07:34,200 --> 00:07:35,960
using extractors would then be

202
00:07:35,960 --> 00:07:37,640
mapped into a target knowledge

203
00:07:37,640 --> 00:07:39,320
graph schema that
we want to use

204
00:07:39,320 --> 00:07:40,140
in the application,

205
00:07:41,015 --> 00:07:42,855
And extraction
results would be

206
00:07:42,855 --> 00:07:45,115
further reconciled, meaning,

207
00:07:45,735 --> 00:07:47,255
quite literally
would basically

208
00:07:47,255 --> 00:07:48,315
connect the dots,

209
00:07:49,340 --> 00:07:51,120
resolve we would
resolve entities

210
00:07:51,180 --> 00:07:53,040
ensuring that there
are no duplicates,

211
00:07:53,100 --> 00:07:54,540
which in the same way would

212
00:07:54,540 --> 00:07:56,060
resolve relationships between

213
00:07:56,060 --> 00:07:57,760
those entities and normalize

214
00:07:57,820 --> 00:07:58,480
the attributes.

215
00:07:58,765 --> 00:07:59,965
At the end of the day,

216
00:07:59,965 --> 00:08:03,345
we would have
a a nice, clean,

217
00:08:03,565 --> 00:08:05,905
consistently duplicated
knowledge graph.

218
00:08:06,650 --> 00:08:09,150
This being an ETL
pipeline, however,

219
00:08:09,690 --> 00:08:12,350
is actually where
big problem is.

220
00:08:14,065 --> 00:08:15,845
Yes. We are getting a nice,

221
00:08:16,545 --> 00:08:18,405
clean knowledge
graph at the end,

222
00:08:19,025 --> 00:08:21,105
but, actually,
it's a knowledge

223
00:08:21,105 --> 00:08:22,410
graph that, again,

224
00:08:22,470 --> 00:08:24,470
is very hard to maintain and

225
00:08:24,470 --> 00:08:25,530
evolve over time,

226
00:08:26,230 --> 00:08:28,090
and it doesn't exactly solve

227
00:08:28,150 --> 00:08:30,630
the data siloing problem that

228
00:08:30,630 --> 00:08:33,425
we wanted to solve
to start with.

229
00:08:33,645 --> 00:08:36,125
Instead, it just creates yet

230
00:08:36,125 --> 00:08:39,025
another data silo.
Now why is that?

231
00:08:39,485 --> 00:08:41,790
Well, if we compare this ETL

232
00:08:41,790 --> 00:08:45,490
approach to some other
known ETL based,

233
00:08:46,430 --> 00:08:49,410
centralized, data
integration platforms,

234
00:08:50,495 --> 00:08:51,775
such as based on the concepts

235
00:08:51,775 --> 00:08:53,555
of data lakes and
data warehouses,

236
00:08:54,255 --> 00:08:56,195
we can see that,

237
00:08:56,415 --> 00:08:57,715
there's a lot of
commonalities.

238
00:08:58,400 --> 00:09:00,240
Okay. The final results might

239
00:09:00,240 --> 00:09:01,540
might look a bit different,

240
00:09:01,760 --> 00:09:03,920
but it's this
centralization of

241
00:09:03,920 --> 00:09:06,215
this whole architecture And,

242
00:09:07,315 --> 00:09:08,515
the the the fact
that the process

243
00:09:08,515 --> 00:09:09,975
is based on ETL,

244
00:09:10,035 --> 00:09:11,975
one directional ETL process,

245
00:09:12,435 --> 00:09:14,560
is what causes the issues.

246
00:09:14,560 --> 00:09:16,020
And the issues are basically,

247
00:09:17,040 --> 00:09:19,140
that we generate a lot of ETL

248
00:09:19,200 --> 00:09:21,300
and ELT code, which,

249
00:09:22,275 --> 00:09:25,895
actually encapsulates
a lot of meaning,

250
00:09:27,395 --> 00:09:28,915
with that we attach
to this data,

251
00:09:28,915 --> 00:09:29,815
a lot of semantics.

252
00:09:30,270 --> 00:09:31,790
And this semantic is only

253
00:09:31,790 --> 00:09:34,210
accessible and manageable for

254
00:09:34,350 --> 00:09:36,210
developers who
build this code.

255
00:09:36,910 --> 00:09:38,845
And the result of it is that

256
00:09:38,925 --> 00:09:40,605
this meaning of data becomes

257
00:09:40,605 --> 00:09:41,485
essentially lost,

258
00:09:41,485 --> 00:09:43,725
and there is no
real end to that

259
00:09:43,725 --> 00:09:47,425
end data ownership,
that would,

260
00:09:47,885 --> 00:09:49,890
allow different,

261
00:09:50,830 --> 00:09:53,870
domain focus teams to be able

262
00:09:53,870 --> 00:09:55,810
to manage and,

263
00:09:56,750 --> 00:09:58,945
govern different data assets

264
00:09:58,945 --> 00:10:00,565
over longer periods of time.

265
00:10:02,145 --> 00:10:03,365
Now what's an alternative?

266
00:10:04,625 --> 00:10:06,490
Following years of experience

267
00:10:06,490 --> 00:10:07,790
with single ad challenges,

268
00:10:08,730 --> 00:10:10,590
we arrived in architecture,

269
00:10:10,650 --> 00:10:12,430
which we call
the knowledge mesh,

270
00:10:13,955 --> 00:10:15,635
which essentially tries to

271
00:10:15,635 --> 00:10:18,615
challenge all
the most important

272
00:10:18,675 --> 00:10:19,875
pain points of those,

273
00:10:21,160 --> 00:10:22,220
previous centralized,

274
00:10:23,160 --> 00:10:24,700
data integration
architectures.

275
00:10:25,880 --> 00:10:28,140
Instead of centralization,

276
00:10:29,335 --> 00:10:31,755
knowledge mesh assumes
a decentralized

277
00:10:32,535 --> 00:10:33,355
data world.

278
00:10:34,535 --> 00:10:38,480
Instead of moving
data to a single

279
00:10:38,540 --> 00:10:40,080
central storage place,

280
00:10:40,780 --> 00:10:43,580
we only virtualize the data at

281
00:10:43,580 --> 00:10:45,515
the query time, at the,

282
00:10:45,975 --> 00:10:48,455
analytic insight
time or put it

283
00:10:48,455 --> 00:10:50,775
generally at
the right time when

284
00:10:50,775 --> 00:10:52,235
the data when the integrated

285
00:10:52,295 --> 00:10:53,275
data is needed.

286
00:10:54,520 --> 00:10:57,020
Instead of developing bespoke

287
00:10:57,480 --> 00:10:58,540
ETL pipelines,

288
00:11:00,600 --> 00:11:02,860
we provide
generic capabilities

289
00:11:03,355 --> 00:11:04,335
driven by metadata,

290
00:11:05,355 --> 00:11:07,775
which can be managed
by nontechnical

291
00:11:08,315 --> 00:11:09,695
subject matter experts.

292
00:11:11,370 --> 00:11:13,290
And instead of focusing on

293
00:11:13,290 --> 00:11:15,870
specific applications
of particular

294
00:11:16,090 --> 00:11:16,990
data assets,

295
00:11:17,930 --> 00:11:20,655
we focus on collecting and

296
00:11:20,655 --> 00:11:22,835
maintaining general knowledge

297
00:11:22,975 --> 00:11:25,315
pertaining to broader
business domains.

298
00:11:26,430 --> 00:11:28,110
So overall knowledge mesh is

299
00:11:28,110 --> 00:11:29,970
this domain
driven architecture

300
00:11:30,590 --> 00:11:32,850
of metadata and
data management

301
00:11:33,150 --> 00:11:34,530
and integration tools,

302
00:11:35,075 --> 00:11:36,615
which are aimed to maximize

303
00:11:36,755 --> 00:11:38,355
the effectiveness of data

304
00:11:38,355 --> 00:11:40,755
utilization in knowledge graph

305
00:11:40,755 --> 00:11:42,455
applications in
different domains.

306
00:11:43,520 --> 00:11:46,160
Let me now walk
you through very

307
00:11:46,160 --> 00:11:47,140
briefly through,

308
00:11:47,600 --> 00:11:49,620
different layers of
this architecture,

309
00:11:50,880 --> 00:11:54,055
to highlight the most
important aspects.

310
00:11:56,595 --> 00:11:58,695
So firstly, there's a uniform

311
00:11:59,075 --> 00:12:02,135
data access and
virtualization layer.

312
00:12:03,510 --> 00:12:06,310
Essentially, data fetchers are

313
00:12:06,310 --> 00:12:07,850
these simple,

314
00:12:08,710 --> 00:12:12,375
services that allow to extract

315
00:12:12,375 --> 00:12:14,315
the data at query time.

316
00:12:14,855 --> 00:12:16,695
They are not basic
they are not

317
00:12:16,695 --> 00:12:19,250
just data extractors
as in this

318
00:12:19,250 --> 00:12:20,950
bay previous ETL,

319
00:12:22,050 --> 00:12:23,490
approach to to
knowledge we have

320
00:12:23,490 --> 00:12:26,310
constructors. They are little

321
00:12:27,475 --> 00:12:29,255
database interfaces,
essentially,

322
00:12:29,315 --> 00:12:31,255
which can take any data source

323
00:12:31,635 --> 00:12:33,655
and expose it using a uniform

324
00:12:34,980 --> 00:12:37,080
data description
language, basically,

325
00:12:37,140 --> 00:12:39,240
a uniform schema
of the source,

326
00:12:39,860 --> 00:12:42,520
and offer a generic
query language,

327
00:12:43,785 --> 00:12:46,185
again, which allows you to

328
00:12:46,185 --> 00:12:50,425
formulate queries
that that are

329
00:12:50,425 --> 00:12:52,150
needed by the application to

330
00:12:52,150 --> 00:12:54,650
satisfy certain,
data requirements.

331
00:12:56,070 --> 00:12:59,210
We use, GraphQL as
this interface,

332
00:12:59,625 --> 00:13:00,425
although, obviously,

333
00:13:00,425 --> 00:13:01,645
this could be achieved,

334
00:13:02,105 --> 00:13:04,125
in number of other ways.

335
00:13:05,465 --> 00:13:07,645
The important part
is that the source,

336
00:13:08,300 --> 00:13:10,700
the the data fetcher
that exposes

337
00:13:10,700 --> 00:13:13,340
the source is agnostic about

338
00:13:13,340 --> 00:13:14,000
the application.

339
00:13:14,700 --> 00:13:15,840
It's the mapping,

340
00:13:16,780 --> 00:13:18,865
explicit declarative mapping

341
00:13:18,925 --> 00:13:23,105
layer that connects
the, the schema,

342
00:13:24,045 --> 00:13:26,670
exposed by the fetcher
to the application

343
00:13:27,290 --> 00:13:29,310
, which provides this essential

344
00:13:29,930 --> 00:13:32,330
glue between the sources and

345
00:13:32,330 --> 00:13:33,550
the application layer.

346
00:13:36,455 --> 00:13:39,595
Now, the one of
the most important,

347
00:13:39,655 --> 00:13:41,195
if not the most important,

348
00:13:42,250 --> 00:13:44,010
aspect of this architecture is

349
00:13:44,010 --> 00:13:46,190
the rich metadata layer.

350
00:13:48,090 --> 00:13:49,310
What is this metadata?

351
00:13:49,370 --> 00:13:51,765
It's basically
the data about,

352
00:13:53,425 --> 00:13:56,245
all the data assets
that are involved,

353
00:13:57,105 --> 00:13:58,885
in the, in the application.

354
00:13:58,945 --> 00:14:00,620
But even more than that,

355
00:14:00,780 --> 00:14:02,880
it's about all
the infrastructural

356
00:14:03,500 --> 00:14:06,160
components that are necessary

357
00:14:07,180 --> 00:14:09,635
to efficiently conduct whole

358
00:14:09,695 --> 00:14:11,615
data integration and data

359
00:14:11,615 --> 00:14:13,875
interpretation
process end to end.

360
00:14:14,015 --> 00:14:15,715
So it's the information about

361
00:14:16,015 --> 00:14:16,995
the data sources,

362
00:14:17,250 --> 00:14:19,190
about the fetchers
that serve them,

363
00:14:19,810 --> 00:14:22,310
about the domains where those

364
00:14:22,690 --> 00:14:24,950
sources are assigned to,

365
00:14:25,455 --> 00:14:28,035
about the mappings,
which map,

366
00:14:28,735 --> 00:14:30,755
the data as served
by the fetchers

367
00:14:31,215 --> 00:14:33,075
to specific domain
vocabularies,

368
00:14:33,295 --> 00:14:34,920
which are also associated with

369
00:14:34,920 --> 00:14:35,740
those domains.

370
00:14:36,600 --> 00:14:38,380
All these components form

371
00:14:39,240 --> 00:14:42,380
essentially a connected
metadata graph,

372
00:14:43,425 --> 00:14:47,565
which provides
a lot of powerful

373
00:14:47,625 --> 00:14:49,165
capabilities to the system.

374
00:14:49,870 --> 00:14:51,630
It allows to catalog different

375
00:14:51,630 --> 00:14:53,150
kind of resources to increase

376
00:14:53,150 --> 00:14:56,670
discoverability.
It opens up a whole

377
00:14:56,670 --> 00:15:00,955
range of, low code,
capabilities,

378
00:15:02,295 --> 00:15:03,675
where we can actually,

379
00:15:04,830 --> 00:15:06,450
instead of writing the spoke,

380
00:15:06,910 --> 00:15:08,750
code support
certain functional

381
00:15:08,750 --> 00:15:10,450
functionalities,
we can offer,

382
00:15:11,230 --> 00:15:12,830
generic solutions which are

383
00:15:12,830 --> 00:15:15,065
consistently driven by this

384
00:15:15,065 --> 00:15:16,765
metadata to achieve certain

385
00:15:16,985 --> 00:15:20,125
goals and support
certain, use cases.

386
00:15:21,140 --> 00:15:22,840
All the metadata
is obviously,

387
00:15:23,780 --> 00:15:25,960
expressed in
standard formats,

388
00:15:27,140 --> 00:15:28,360
standard metamodels,

389
00:15:28,820 --> 00:15:30,375
which further increases

390
00:15:30,435 --> 00:15:32,455
interoperability
between humans

391
00:15:32,515 --> 00:15:33,575
and machine agents.

392
00:15:37,050 --> 00:15:39,610
All the metadata, as
mentioned before,

393
00:15:39,610 --> 00:15:41,390
is, is,

394
00:15:42,250 --> 00:15:44,805
organized in a domain
oriented fashion.

395
00:15:45,045 --> 00:15:46,345
Now what does it mean?

396
00:15:46,405 --> 00:15:47,865
What is a domain read?

397
00:15:48,405 --> 00:15:51,125
A domain is
a predefined collection

398
00:15:51,125 --> 00:15:54,950
of assets that make sense,

399
00:15:55,410 --> 00:15:58,450
and are particularly useful in

400
00:15:58,450 --> 00:16:00,690
the context of some range of

401
00:16:00,690 --> 00:16:03,325
applications which
would jointly,

402
00:16:04,225 --> 00:16:06,485
collect in a in
under an umbrella

403
00:16:06,545 --> 00:16:08,545
of some, business domain.

404
00:16:08,545 --> 00:16:10,065
This kind of business domain

405
00:16:10,065 --> 00:16:11,670
could be a compliance or could

406
00:16:11,670 --> 00:16:13,430
be cyber or perhaps could be

407
00:16:13,430 --> 00:16:15,670
health care. In any case,

408
00:16:15,670 --> 00:16:18,310
a domain consists of a domain

409
00:16:18,310 --> 00:16:19,885
vocabulary. So the collection

410
00:16:19,885 --> 00:16:21,745
of terms which we
use to describe

411
00:16:21,805 --> 00:16:24,205
all important, data relevant

412
00:16:24,205 --> 00:16:25,505
data within this domain.

413
00:16:26,500 --> 00:16:28,420
It contains of a collection of

414
00:16:28,420 --> 00:16:30,420
sources which are
catalogued for

415
00:16:30,420 --> 00:16:31,480
browsing, for discovery.

416
00:16:33,885 --> 00:16:36,865
It also contains
mappings that,

417
00:16:37,885 --> 00:16:41,325
again, generically
map the data

418
00:16:41,325 --> 00:16:43,760
from this domain to the domain

419
00:16:43,760 --> 00:16:46,820
vocabulary and
possibly also a bit,

420
00:16:47,920 --> 00:16:49,280
a collection of of,

421
00:16:49,760 --> 00:16:53,025
domain specific algorithms or

422
00:16:53,025 --> 00:16:54,325
basically some intelligence

423
00:16:54,465 --> 00:16:56,385
which is already built in into

424
00:16:56,385 --> 00:16:58,885
that domain. For instance,

425
00:17:00,100 --> 00:17:02,580
algorithms that can that allow

426
00:17:02,580 --> 00:17:04,280
you to perform
entity resolution

427
00:17:04,500 --> 00:17:05,240
on entities,

428
00:17:06,260 --> 00:17:08,840
within specifically
within this domain.

429
00:17:11,445 --> 00:17:13,785
As a result of of of offering

430
00:17:13,925 --> 00:17:15,365
those kind of of
building those

431
00:17:15,365 --> 00:17:16,825
kind of business domains,

432
00:17:17,480 --> 00:17:19,880
the process of
constructing a specific

433
00:17:19,880 --> 00:17:21,660
application and and,

434
00:17:22,280 --> 00:17:24,280
developing a a knowledge graph

435
00:17:24,280 --> 00:17:26,485
for this kind of
application can be,

436
00:17:26,485 --> 00:17:28,505
to a large extent, automated.

437
00:17:32,885 --> 00:17:35,350
It's, so once the application

438
00:17:35,490 --> 00:17:37,430
is being created in
specific domains,

439
00:17:38,050 --> 00:17:40,610
we can automatically leverage

440
00:17:40,610 --> 00:17:41,810
the metadata, the whole

441
00:17:41,810 --> 00:17:43,590
understanding of which sources

442
00:17:43,735 --> 00:17:46,215
can be used, how they map to

443
00:17:46,215 --> 00:17:47,355
this domain vocabulary,

444
00:17:48,295 --> 00:17:49,435
and how they should,

445
00:17:49,895 --> 00:17:51,595
how the data from
these sources

446
00:17:51,815 --> 00:17:53,275
should be
essentially processed

447
00:17:53,600 --> 00:17:55,520
so that the knowledge
graph powering,

448
00:17:55,520 --> 00:17:56,900
fueling this application,

449
00:17:57,040 --> 00:17:58,740
this domain can be,

450
00:17:59,360 --> 00:18:00,660
constructed relatively,

451
00:18:02,155 --> 00:18:05,195
seamlessly with
a reasonably low

452
00:18:05,195 --> 00:18:07,375
effort from the application
developer.

453
00:18:08,155 --> 00:18:10,620
On the way, we collect we are

454
00:18:10,620 --> 00:18:12,960
able to collect another layer

455
00:18:13,100 --> 00:18:15,520
of metadata. This
is a metadata

456
00:18:15,660 --> 00:18:18,285
about where each data
item comes from.

457
00:18:20,205 --> 00:18:21,965
What do we know
about the source

458
00:18:21,965 --> 00:18:22,945
it came from?

459
00:18:23,085 --> 00:18:25,085
With what confidence and with

460
00:18:25,085 --> 00:18:28,340
what method, this
specific piece of,

461
00:18:28,740 --> 00:18:30,280
data has been extracted,

462
00:18:31,220 --> 00:18:35,205
and how it was
processed later on to,

463
00:18:35,205 --> 00:18:36,585
for instance, resolve,

464
00:18:37,125 --> 00:18:39,465
one entity against
against the other.

465
00:18:40,405 --> 00:18:41,465
All these additional,

466
00:18:42,005 --> 00:18:45,020
layers of metadata
are something

467
00:18:45,800 --> 00:18:49,340
that essentially allows us to

468
00:18:50,600 --> 00:18:54,195
to offer higher
quality guarantees,

469
00:18:55,375 --> 00:18:56,995
to the application consumers.

470
00:18:57,855 --> 00:19:00,115
So this is essentially
the process

471
00:19:00,175 --> 00:19:04,690
of extracting
knowledge from raw

472
00:19:04,690 --> 00:19:06,630
data where
the knowledge basically

473
00:19:06,690 --> 00:19:09,190
comes into the picture
at the moment

474
00:19:09,455 --> 00:19:10,915
where you can start building

475
00:19:11,215 --> 00:19:13,395
trust and start
building confidence

476
00:19:14,015 --> 00:19:17,950
that that the raw data was,

477
00:19:18,570 --> 00:19:21,370
was was was sourced and was

478
00:19:21,370 --> 00:19:24,110
processed in a way
that ensures

479
00:19:25,015 --> 00:19:26,615
high quality information at

480
00:19:26,615 --> 00:19:27,675
the end of the process.

481
00:19:31,575 --> 00:19:35,140
Now when we add these two

482
00:19:35,140 --> 00:19:36,680
different layers of metadata,

483
00:19:37,300 --> 00:19:38,280
which I mentioned,

484
00:19:39,060 --> 00:19:40,680
in the previous
slides together,

485
00:19:41,140 --> 00:19:43,665
the metadata and the data

486
00:19:43,805 --> 00:19:45,505
collected from
the data sources

487
00:19:46,445 --> 00:19:49,665
and the metadata
about the whole

488
00:19:51,450 --> 00:19:53,390
about the whole
infrastructure,

489
00:19:53,850 --> 00:19:55,870
data integration
infrastructure,

490
00:19:57,210 --> 00:20:00,075
we finally obtain this,

491
00:20:01,415 --> 00:20:04,215
this complex twofold knowledge

492
00:20:04,215 --> 00:20:07,195
graph or as it's
sometimes called,

493
00:20:07,410 --> 00:20:08,950
a full data fabric.

494
00:20:09,890 --> 00:20:13,330
It is exactly
the structure that

495
00:20:13,330 --> 00:20:15,190
is necessary for efficiently

496
00:20:15,490 --> 00:20:17,845
utilizing all
available information

497
00:20:18,705 --> 00:20:22,165
and automating all
the data processes,

498
00:20:24,305 --> 00:20:25,275
that we want to,

499
00:20:26,000 --> 00:20:28,400
that we need to carry out in

500
00:20:28,400 --> 00:20:30,880
order to gain insights and get

501
00:20:30,880 --> 00:20:32,660
knowledge out of
the originally

502
00:20:32,800 --> 00:20:34,820
disconnected data sources.

503
00:20:35,725 --> 00:20:40,145
So to summarize,

504
00:20:40,845 --> 00:20:43,245
knowledge mesh is this domain

505
00:20:43,245 --> 00:20:46,000
centric data integration and

506
00:20:46,000 --> 00:20:47,700
virtualization architecture

507
00:20:48,640 --> 00:20:50,660
consisting of
a range of advanced

508
00:20:51,255 --> 00:20:53,015
data management,
data processing,

509
00:20:53,015 --> 00:20:56,155
data cataloging tools
working together,

510
00:20:57,655 --> 00:21:02,930
to, to basically help automate

511
00:21:02,930 --> 00:21:05,410
and help manage all the steps

512
00:21:05,410 --> 00:21:08,335
required to build efficiently

513
00:21:08,475 --> 00:21:11,195
build up to date
knowledge graphs

514
00:21:11,195 --> 00:21:13,055
from disconnected distributed

515
00:21:13,195 --> 00:21:14,095
data sources.

516
00:21:15,360 --> 00:21:18,100
It, thus maximizes
the effectiveness

517
00:21:18,240 --> 00:21:20,320
of data utilization
in knowledge

518
00:21:20,320 --> 00:21:21,140
graph applications.

519
00:21:22,615 --> 00:21:25,735
And, for those and perhaps

520
00:21:25,735 --> 00:21:28,395
familiar with so called
FAIR principles,

521
00:21:28,695 --> 00:21:29,835
principles basically,

522
00:21:31,270 --> 00:21:34,650
adopted increasingly
adopted by

523
00:21:34,710 --> 00:21:37,450
a large data intensive
enterprise.

524
00:21:38,695 --> 00:21:40,215
Knowledge mesh is
really nothing

525
00:21:40,215 --> 00:21:43,435
else but metadata first

526
00:21:44,935 --> 00:21:47,760
architecture that is intended

527
00:21:48,140 --> 00:21:52,320
and it allows to make
the data findable,

528
00:21:53,020 --> 00:21:56,245
accessible, interpretable,
and reusable.

529
00:21:57,745 --> 00:22:00,005
And with this,
I'd like to finish

530
00:22:00,145 --> 00:22:01,845
and together with David,

531
00:22:02,145 --> 00:22:04,005
take any questions
you might have.

532
00:22:04,205 --> 00:22:08,385
Thank you for your attention.

