1
00:00:04,500 --> 00:00:05,940
Hello. Connected Data World

2
00:00:05,940 --> 00:00:06,900
twenty twenty one.

3
00:00:06,900 --> 00:00:08,420
My name is Peter Kaloroumakis,

4
00:00:08,420 --> 00:00:10,180
and I'm happy to
get to present

5
00:00:10,180 --> 00:00:11,880
the defend project
to you today.

6
00:00:12,055 --> 00:00:13,975
A little background on me.

7
00:00:13,975 --> 00:00:16,715
I work for, MITRE
Corporation.

8
00:00:17,175 --> 00:00:19,035
We're based in
the United States,

9
00:00:19,310 --> 00:00:20,770
and we focus on,

10
00:00:21,390 --> 00:00:22,990
research and
development problems

11
00:00:22,990 --> 00:00:24,610
for our sponsors, primarily,

12
00:00:25,550 --> 00:00:26,210
the government.

13
00:00:26,830 --> 00:00:28,370
Before I came to MITRE,

14
00:00:29,105 --> 00:00:30,545
I worked in commercial product

15
00:00:30,545 --> 00:00:34,805
development, and I,
designed and built,

16
00:00:35,265 --> 00:00:36,785
malware detection products,

17
00:00:37,025 --> 00:00:39,580
for network
security purposes.

18
00:00:40,680 --> 00:00:42,200
So today, I'm gonna give you,

19
00:00:42,520 --> 00:00:44,620
an overview of sort
of the motivation

20
00:00:44,840 --> 00:00:45,880
behind this project,

21
00:00:45,880 --> 00:00:47,955
what problems we were
trying to solve.

22
00:00:48,815 --> 00:00:51,795
I'll talk about,
what we built,

23
00:00:51,935 --> 00:00:53,875
a little bit on
how we build it,

24
00:00:54,015 --> 00:00:55,775
and then also talk about how

25
00:00:55,775 --> 00:00:57,400
some people are
using it to help

26
00:00:57,400 --> 00:00:59,340
better organize
their cybersecurity

27
00:00:59,720 --> 00:01:02,300
defenses. So let's
get started.

28
00:01:05,055 --> 00:01:07,475
So first, I wanna get
into the motivation

29
00:01:07,615 --> 00:01:08,595
for this project.

30
00:01:11,135 --> 00:01:14,050
And before I start
talking about defend,

31
00:01:14,270 --> 00:01:15,870
one of the things I have to

32
00:01:15,870 --> 00:01:18,770
orient you to is
another MITRE project.

33
00:01:19,935 --> 00:01:21,795
It's called MITRE ATT and CK.

34
00:01:22,255 --> 00:01:25,135
MITRE ATT and CK
was a is is a very

35
00:01:25,135 --> 00:01:29,950
popular framework
for characterizing,

36
00:01:31,530 --> 00:01:33,150
what specific things,

37
00:01:33,770 --> 00:01:36,095
cybersecurity adversaries are

38
00:01:36,095 --> 00:01:38,595
doing when they break
into a network.

39
00:01:39,695 --> 00:01:42,095
This model was built by MITRE

40
00:01:42,095 --> 00:01:43,615
over a number of years and

41
00:01:43,615 --> 00:01:44,755
publicly released,

42
00:01:45,120 --> 00:01:47,600
and it's really
taken the industry

43
00:01:47,600 --> 00:01:48,420
by storm.

44
00:01:48,640 --> 00:01:51,040
And the result effect is that

45
00:01:51,040 --> 00:01:53,495
when when
cybersecurity products

46
00:01:54,115 --> 00:01:57,015
are able to, detect
something bad,

47
00:01:57,635 --> 00:01:59,495
they characterize it using

48
00:01:59,700 --> 00:02:01,860
terminology from
this MITRE ATT

49
00:02:01,860 --> 00:02:02,600
and CK project.

50
00:02:03,620 --> 00:02:07,640
So, once that was, deployed,

51
00:02:07,780 --> 00:02:09,375
sort of it got out there,

52
00:02:09,375 --> 00:02:10,595
it became very successful,

53
00:02:10,895 --> 00:02:13,215
a sort of natural
question arose,

54
00:02:13,215 --> 00:02:14,195
which is sort of,

55
00:02:14,415 --> 00:02:16,360
could you describe what

56
00:02:16,360 --> 00:02:18,520
cybersecurity defenders do and

57
00:02:18,520 --> 00:02:20,300
as much fidelity as we,

58
00:02:20,840 --> 00:02:23,100
describe the cyber
the attackers?

59
00:02:23,615 --> 00:02:25,455
So we basically proposed this

60
00:02:25,455 --> 00:02:28,275
project to help
organize the defensive

61
00:02:28,415 --> 00:02:30,515
space the same way that,

62
00:02:31,120 --> 00:02:32,260
MITRE had organized,

63
00:02:32,800 --> 00:02:34,340
the offensive space.

64
00:02:35,360 --> 00:02:37,520
So this defensive
space is huge.

65
00:02:37,520 --> 00:02:39,280
And if you think
about it in two

66
00:02:39,280 --> 00:02:41,125
main buckets, first,

67
00:02:41,125 --> 00:02:42,665
sort of technical policies.

68
00:02:43,045 --> 00:02:44,325
There are thousands of these

69
00:02:44,325 --> 00:02:46,005
technical policies
which describe

70
00:02:46,005 --> 00:02:48,140
how cybersecurity protections

71
00:02:48,140 --> 00:02:50,400
can be applied to IT
infrastructures.

72
00:02:51,420 --> 00:02:53,340
And, you know,
there's a lot of

73
00:02:53,340 --> 00:02:54,320
material there.

74
00:02:54,380 --> 00:02:56,655
The top image at the right is

75
00:02:56,655 --> 00:02:58,335
sort of documents
that describe

76
00:02:58,335 --> 00:03:00,755
how to do this grouped
by the general,

77
00:03:01,455 --> 00:03:02,435
sort of domain.

78
00:03:02,735 --> 00:03:04,175
And in each of
those documents,

79
00:03:04,175 --> 00:03:06,550
there might be hundreds of

80
00:03:06,550 --> 00:03:08,170
recommendations
or requirements

81
00:03:08,310 --> 00:03:09,850
that people have
to implement.

82
00:03:10,790 --> 00:03:13,030
The other big component
of this space,

83
00:03:13,030 --> 00:03:15,265
the other major bucket is,

84
00:03:15,825 --> 00:03:17,205
cybersecurity vendors.

85
00:03:17,745 --> 00:03:20,005
So
the cybersecurity marketplace

86
00:03:20,305 --> 00:03:21,365
is pretty large.

87
00:03:22,065 --> 00:03:23,365
It's a pretty big market.

88
00:03:24,350 --> 00:03:26,210
There are thousands
of these vendors.

89
00:03:27,070 --> 00:03:28,270
Some of these vendors have

90
00:03:28,270 --> 00:03:29,250
multiple products,

91
00:03:29,870 --> 00:03:31,730
and some of those products,

92
00:03:32,285 --> 00:03:34,465
performed dozens of
discrete functions.

93
00:03:34,925 --> 00:03:37,405
So this really
quickly gets into,

94
00:03:37,725 --> 00:03:39,325
a high dimensional
space that's

95
00:03:39,325 --> 00:03:41,800
difficult to
manage if you're,

96
00:03:42,340 --> 00:03:44,260
you know, organizing your your

97
00:03:44,260 --> 00:03:45,860
defenses and you have to keep

98
00:03:45,860 --> 00:03:47,320
track of all of
this information.

99
00:03:49,095 --> 00:03:50,775
So in order to address all of

100
00:03:50,775 --> 00:03:51,515
this complexity,

101
00:03:52,215 --> 00:03:54,955
we wanted to build
a conceptual

102
00:03:55,095 --> 00:03:57,595
model to help
organize this space.

103
00:03:58,730 --> 00:04:00,410
There's a lot of
challenges with

104
00:04:00,410 --> 00:04:01,550
trying to do this,

105
00:04:02,490 --> 00:04:04,010
particularly because it's such

106
00:04:04,010 --> 00:04:06,430
a complex and nuanced space.

107
00:04:07,835 --> 00:04:09,855
One of
the challenges specifically

108
00:04:10,075 --> 00:04:11,675
with cybersecurity
is the amount

109
00:04:11,675 --> 00:04:14,715
of complexity and
nuance in all

110
00:04:14,715 --> 00:04:16,575
the terminology that's used.

111
00:04:17,140 --> 00:04:18,580
And one of the things we asked

112
00:04:18,580 --> 00:04:20,180
ourselves was, how do we know

113
00:04:20,180 --> 00:04:21,540
if we were successful
in building

114
00:04:21,540 --> 00:04:22,600
a useful model?

115
00:04:23,140 --> 00:04:24,580
We felt like if we could use

116
00:04:24,580 --> 00:04:26,120
the same model for multiple

117
00:04:26,465 --> 00:04:27,285
target audiences,

118
00:04:27,505 --> 00:04:28,785
that would give us
some indication

119
00:04:28,785 --> 00:04:31,025
that we hit that
abstraction or

120
00:04:31,025 --> 00:04:33,125
semantic sweet spot,
if you will.

121
00:04:33,830 --> 00:04:35,830
So imagine three
sort of target

122
00:04:35,830 --> 00:04:37,210
audiences. First,

123
00:04:37,670 --> 00:04:39,670
cybersecurity
architects who are

124
00:04:39,670 --> 00:04:41,370
managing these
complex portfolios.

125
00:04:41,795 --> 00:04:43,255
They really have to understand

126
00:04:43,875 --> 00:04:46,055
what specific functions
all of these,

127
00:04:46,675 --> 00:04:48,195
products and offerings they're

128
00:04:48,195 --> 00:04:51,550
purchasing, do in
great detail.

129
00:04:52,490 --> 00:04:54,730
Another target
audience is what

130
00:04:54,730 --> 00:04:56,750
we call adversary emulation,

131
00:04:57,450 --> 00:04:59,055
where they're doing a pen test

132
00:04:59,055 --> 00:05:00,895
on a system, and they have to

133
00:05:00,895 --> 00:05:02,255
really understand how these

134
00:05:02,255 --> 00:05:04,755
things work in order
to have a successful

135
00:05:04,895 --> 00:05:06,035
, useful pen test.

136
00:05:07,110 --> 00:05:09,050
Imagine a third
audience for this.

137
00:05:09,830 --> 00:05:11,750
Let's say
a cybersecurity investor

138
00:05:11,750 --> 00:05:13,110
who's trying to decide whether

139
00:05:13,110 --> 00:05:14,890
or not they should
invest in a product.

140
00:05:15,455 --> 00:05:16,975
They have to understand what

141
00:05:16,975 --> 00:05:18,335
problem is being solved,

142
00:05:18,335 --> 00:05:19,775
has that been solved before,

143
00:05:19,775 --> 00:05:21,295
and is this being
done in a new

144
00:05:21,295 --> 00:05:23,055
and novel way. Right?

145
00:05:23,055 --> 00:05:24,655
So we felt like if we could

146
00:05:24,655 --> 00:05:26,710
address these
multiple audiences,

147
00:05:26,710 --> 00:05:27,990
that would give us
some indication

148
00:05:27,990 --> 00:05:29,370
we were on the right track.

149
00:05:29,750 --> 00:05:31,930
So we set about
building this,

150
00:05:32,390 --> 00:05:34,095
and three years,

151
00:05:34,475 --> 00:05:37,035
in the in the making
with a very

152
00:05:37,035 --> 00:05:39,275
small team, we
put together what

153
00:05:39,275 --> 00:05:41,195
you see today. We
call it the defend

154
00:05:41,195 --> 00:05:42,175
knowledge graph.

155
00:05:42,560 --> 00:05:45,200
You can go to defend dot mitre

156
00:05:45,200 --> 00:05:48,180
dot org to browse the graph,

157
00:05:48,560 --> 00:05:51,005
and it renders
portions of the,

158
00:05:51,245 --> 00:05:52,945
graph in the underlying model

159
00:05:53,325 --> 00:05:55,565
for people to
start to use it.

160
00:05:55,565 --> 00:05:58,100
It's very early
stage, and there's,

161
00:05:58,720 --> 00:05:59,760
some rough edges,

162
00:05:59,760 --> 00:06:01,600
but we felt like it
was at the point

163
00:06:01,600 --> 00:06:03,280
where it made sense to open it

164
00:06:03,280 --> 00:06:04,260
up for collaboration.

165
00:06:05,280 --> 00:06:07,905
We built the graph by,

166
00:06:09,405 --> 00:06:11,585
analyzing lots of open source

167
00:06:11,645 --> 00:06:13,565
materials that explain how

168
00:06:13,565 --> 00:06:15,165
cybersecurity
technologies work

169
00:06:15,165 --> 00:06:17,260
in detail. This information is

170
00:06:17,260 --> 00:06:18,560
very difficult to get,

171
00:06:18,700 --> 00:06:20,620
but we found
a dataset that was

172
00:06:20,620 --> 00:06:22,140
sort of hiding
in plain sight.

173
00:06:22,140 --> 00:06:23,580
And it didn't look like anyone

174
00:06:23,580 --> 00:06:25,040
else had
actually systematically

175
00:06:25,340 --> 00:06:27,665
analyzed this corpus before.

176
00:06:28,445 --> 00:06:29,325
And these were,

177
00:06:30,445 --> 00:06:32,465
patents that were
filed by these

178
00:06:32,605 --> 00:06:35,265
cybersecurity
vendors that explain

179
00:06:35,325 --> 00:06:38,030
how their product
works or what

180
00:06:38,030 --> 00:06:40,610
specific problem it's
going to solve.

181
00:06:41,630 --> 00:06:43,070
There's a lot of diversity in

182
00:06:43,070 --> 00:06:44,450
the patent corpus.

183
00:06:45,245 --> 00:06:46,685
And for us, the challenge was

184
00:06:46,685 --> 00:06:47,965
how do you take a twenty or

185
00:06:47,965 --> 00:06:50,365
thirty page document and turn

186
00:06:50,365 --> 00:06:52,125
it into two, three,

187
00:06:52,125 --> 00:06:54,450
or four words that
really crisply

188
00:06:54,750 --> 00:06:56,930
explain what it's doing.

189
00:06:57,710 --> 00:06:59,650
So we developed a methodology

190
00:06:59,870 --> 00:07:02,725
which lets us
systematically do that,

191
00:07:02,865 --> 00:07:04,705
and that's part
of how we built

192
00:07:04,705 --> 00:07:06,405
the the overall model.

193
00:07:07,025 --> 00:07:08,625
The picture you see at the top

194
00:07:08,625 --> 00:07:10,560
left is the most prominent

195
00:07:10,860 --> 00:07:12,880
taxonomy inside of defend.

196
00:07:13,420 --> 00:07:15,280
Those are the defensive
countermeasures,

197
00:07:16,445 --> 00:07:18,785
of course, arranged
by, type hierarchy.

198
00:07:19,325 --> 00:07:21,805
And what we did was we render

199
00:07:21,805 --> 00:07:24,765
it as sort of
a tabular view in

200
00:07:24,765 --> 00:07:27,230
order to get folks interested

201
00:07:27,450 --> 00:07:29,130
in the content that's embedded

202
00:07:29,130 --> 00:07:29,950
in the graph.

203
00:07:30,570 --> 00:07:32,750
We also want
people to use this

204
00:07:33,505 --> 00:07:36,405
graph to do more
sophisticated modeling,

205
00:07:36,865 --> 00:07:40,145
and we built it using standard

206
00:07:40,145 --> 00:07:42,230
ontology tools in
order to support

207
00:07:42,230 --> 00:07:43,210
those use cases.

208
00:07:44,070 --> 00:07:46,150
One obvious first
use case is,

209
00:07:46,150 --> 00:07:47,510
okay. Now that I've got these

210
00:07:47,510 --> 00:07:50,090
defensive countermeasures
identified,

211
00:07:51,235 --> 00:07:53,715
what offensive techniques are

212
00:07:53,715 --> 00:07:55,075
are those going to address?

213
00:07:55,075 --> 00:07:57,815
So you see this
picture of defend

214
00:07:57,875 --> 00:07:59,255
connected to attack,

215
00:07:59,950 --> 00:08:01,250
with these red lines.

216
00:08:01,710 --> 00:08:02,910
And then you see this call out

217
00:08:02,910 --> 00:08:04,750
in the middle here with this

218
00:08:04,750 --> 00:08:06,930
idea that we call
digital artifacts.

219
00:08:07,805 --> 00:08:09,665
So instead of just directly

220
00:08:09,805 --> 00:08:12,385
relating
the defensive techniques

221
00:08:12,445 --> 00:08:13,745
to the offensive techniques,

222
00:08:14,285 --> 00:08:15,725
we do it through inference and

223
00:08:15,725 --> 00:08:17,690
reasoning through
this intermediate

224
00:08:17,910 --> 00:08:19,830
model. And I'm
gonna talk about

225
00:08:19,830 --> 00:08:21,990
that in much more detail on

226
00:08:21,990 --> 00:08:23,290
the next slide here.

227
00:08:23,510 --> 00:08:26,195
So what you're
gonna see is this

228
00:08:26,355 --> 00:08:28,195
sort of cartoon view blown up

229
00:08:28,195 --> 00:08:30,115
with specific
examples of the types

230
00:08:30,115 --> 00:08:32,295
of information,
we're tracking.

231
00:08:33,635 --> 00:08:35,820
So, the same view,

232
00:08:35,820 --> 00:08:38,080
you've got attack
on the left,

233
00:08:38,380 --> 00:08:39,500
defend on the right.

234
00:08:39,500 --> 00:08:40,460
And in the middle,

235
00:08:40,460 --> 00:08:42,300
you see what we
call the digital

236
00:08:42,300 --> 00:08:43,440
artifact ontology.

237
00:08:44,205 --> 00:08:46,685
So this is describing
all the sort

238
00:08:46,685 --> 00:08:48,865
of basic computer
science concepts,

239
00:08:49,805 --> 00:08:51,325
that someone taking computer

240
00:08:51,325 --> 00:08:53,185
science one zero one might be

241
00:08:53,530 --> 00:08:54,430
learning about.

242
00:08:56,090 --> 00:08:57,630
And, surprisingly,

243
00:08:58,170 --> 00:09:00,590
we didn't find a model
that existed,

244
00:09:01,495 --> 00:09:03,415
that met our needs to for

245
00:09:03,415 --> 00:09:04,875
cybersecurity purposes.

246
00:09:04,935 --> 00:09:06,535
So we ended up having to take

247
00:09:06,535 --> 00:09:08,135
a lot of basic
computer science

248
00:09:08,135 --> 00:09:11,090
terminology and make it a bit

249
00:09:11,090 --> 00:09:11,990
more specific,

250
00:09:13,170 --> 00:09:14,710
because at the end
of the day,

251
00:09:14,930 --> 00:09:15,830
a lot of times,

252
00:09:16,130 --> 00:09:17,750
the attackers are abusing

253
00:09:18,435 --> 00:09:19,895
functionality in
the computer,

254
00:09:20,195 --> 00:09:21,795
and we end up
needing to be more

255
00:09:21,795 --> 00:09:23,955
precise about what specific

256
00:09:23,955 --> 00:09:26,860
functionality they're
manipulating.

257
00:09:27,560 --> 00:09:29,740
So you see on the left,

258
00:09:29,800 --> 00:09:31,980
a few of these
offensive techniques,

259
00:09:32,600 --> 00:09:33,960
exploitation techniques,

260
00:09:33,960 --> 00:09:36,045
and then another exploitation

261
00:09:36,985 --> 00:09:38,685
technique called,
process injection.

262
00:09:39,865 --> 00:09:41,385
And and then on the right,

263
00:09:41,385 --> 00:09:42,585
you've got this defensive

264
00:09:42,585 --> 00:09:44,105
technique process code segment

265
00:09:44,105 --> 00:09:45,820
verification.
And both of these

266
00:09:45,820 --> 00:09:48,000
are mapped to
this intermediate

267
00:09:48,220 --> 00:09:50,320
concept called a process
code segment.

268
00:09:50,780 --> 00:09:52,720
And the relationship
on the offensive

269
00:09:52,780 --> 00:09:54,435
side is that these techniques

270
00:09:54,435 --> 00:09:56,535
are modifying this concept,

271
00:09:56,915 --> 00:09:58,755
and then on
the defensive side,

272
00:09:58,755 --> 00:10:00,535
you're verifying
this concept.

273
00:10:01,610 --> 00:10:03,450
You can see that
a process code

274
00:10:03,450 --> 00:10:05,870
segment is a type
of code segment,

275
00:10:06,250 --> 00:10:07,870
which is a digital artifact.

276
00:10:08,315 --> 00:10:10,075
And then you see some object

277
00:10:10,075 --> 00:10:11,055
property relationships.

278
00:10:11,195 --> 00:10:12,715
For example, a process running

279
00:10:12,715 --> 00:10:14,555
on your computer
contains process

280
00:10:14,555 --> 00:10:16,530
code segments. The process

281
00:10:16,590 --> 00:10:17,970
originated from an executable

282
00:10:18,110 --> 00:10:19,010
binary file,

283
00:10:19,710 --> 00:10:21,490
that contains code segments.

284
00:10:21,790 --> 00:10:23,570
And when the process
is running

285
00:10:23,715 --> 00:10:24,615
on your system,

286
00:10:24,835 --> 00:10:26,695
it loads those executable,

287
00:10:28,115 --> 00:10:29,875
parts of the file into memory

288
00:10:29,875 --> 00:10:31,395
and then executes those.

289
00:10:31,395 --> 00:10:32,675
And then sometimes those are

290
00:10:32,675 --> 00:10:34,540
the parts that attackers will

291
00:10:34,920 --> 00:10:37,100
overwrite with code
that you don't want.

292
00:10:37,160 --> 00:10:38,940
So if you wanna
check the integrity

293
00:10:39,400 --> 00:10:41,820
of your processes in
their memory space,

294
00:10:42,355 --> 00:10:44,195
you might try to
verify process

295
00:10:44,195 --> 00:10:46,035
code segments. So once we have

296
00:10:46,035 --> 00:10:47,495
those definitions defined,

297
00:10:48,195 --> 00:10:50,435
we can then infer
that this one

298
00:10:50,435 --> 00:10:53,220
defensive technique
might detect

299
00:10:53,280 --> 00:10:55,860
this particular
offensive technique,

300
00:10:57,040 --> 00:10:58,580
without needing to directly

301
00:10:58,720 --> 00:10:59,700
declare that.

302
00:11:00,095 --> 00:11:02,255
And because of the amount of

303
00:11:02,255 --> 00:11:03,715
information we're
dealing with,

304
00:11:03,775 --> 00:11:05,715
this ends up being
a much more scalable,

305
00:11:05,855 --> 00:11:07,775
manageable approach that lets

306
00:11:07,775 --> 00:11:08,995
you reason about,

307
00:11:09,950 --> 00:11:11,330
the the the whole graph,

308
00:11:12,030 --> 00:11:13,970
rather than trying to manually

309
00:11:14,030 --> 00:11:15,570
go through and
map everything,

310
00:11:16,510 --> 00:11:19,205
accordingly. So
this is sort of,

311
00:11:19,445 --> 00:11:21,125
an advancement in the state of

312
00:11:21,125 --> 00:11:22,725
the art for us, which is,

313
00:11:23,285 --> 00:11:24,965
to to do these sorts of things

314
00:11:24,965 --> 00:11:27,500
by inference
instead of manually

315
00:11:27,500 --> 00:11:29,440
mapping things
one to another.

316
00:11:30,300 --> 00:11:31,600
So next,

317
00:11:31,660 --> 00:11:33,260
I'm gonna show you what this

318
00:11:33,260 --> 00:11:36,765
information looks
like in the actual

319
00:11:37,225 --> 00:11:39,945
tool. So if you
go to defend at

320
00:11:39,945 --> 00:11:42,505
mitre dot org,
you can, load up,

321
00:11:43,240 --> 00:11:45,080
the home page, and you'll see

322
00:11:45,080 --> 00:11:46,680
this countermeasure technique

323
00:11:46,680 --> 00:11:48,940
taxonomy that that
you see here.

324
00:11:49,480 --> 00:11:50,540
Across the top,

325
00:11:50,600 --> 00:11:52,940
we have what we call
defensive tactics.

326
00:11:53,865 --> 00:11:55,725
Those are sort
of the maneuvers

327
00:11:55,865 --> 00:11:58,025
that a defender can perform in

328
00:11:58,025 --> 00:11:59,885
response to
adversary activity.

329
00:12:00,825 --> 00:12:03,090
And then beneath that are what

330
00:12:03,090 --> 00:12:05,170
we call defensive
techniques or

331
00:12:05,170 --> 00:12:06,310
countermeasure techniques.

332
00:12:06,930 --> 00:12:07,750
These are,

333
00:12:09,525 --> 00:12:11,045
the the highest order ones at

334
00:12:11,045 --> 00:12:13,545
the at the second
level and then,

335
00:12:14,165 --> 00:12:17,285
more specific types
of of the technique

336
00:12:17,285 --> 00:12:18,965
as you go down the column.

337
00:12:18,965 --> 00:12:22,330
So user behavior analysis has

338
00:12:22,390 --> 00:12:23,830
more specific types of user

339
00:12:23,830 --> 00:12:24,630
behavior analysis,

340
00:12:24,630 --> 00:12:26,550
and process analysis has more

341
00:12:26,550 --> 00:12:28,390
specific types of
process analysis.

342
00:12:28,390 --> 00:12:29,895
One of which is is the example

343
00:12:29,895 --> 00:12:31,195
I gave in the slide,

344
00:12:31,335 --> 00:12:32,955
process code segment
verification.

345
00:12:33,335 --> 00:12:36,395
Right? And this number
we're showing,

346
00:12:36,620 --> 00:12:38,300
we found six public references

347
00:12:38,300 --> 00:12:40,080
that indicate there
are technologies

348
00:12:40,300 --> 00:12:42,380
that do this. So
we believe it's

349
00:12:42,380 --> 00:12:43,840
worth including into,

350
00:12:44,140 --> 00:12:45,200
the knowledge base.

351
00:12:45,575 --> 00:12:47,595
So if I click this, you get,

352
00:12:48,775 --> 00:12:50,955
what we call the knowledge
base article.

353
00:12:51,495 --> 00:12:54,075
This is all encoded,
in the ontology.

354
00:12:55,240 --> 00:12:57,160
And there you have a crisp

355
00:12:57,160 --> 00:13:00,060
definition of of
this concept.

356
00:13:00,360 --> 00:13:02,140
We wanna be able
to ask a vendor,

357
00:13:02,200 --> 00:13:03,900
do you do this? Yes or no?

358
00:13:04,475 --> 00:13:05,955
You get an explanation of how

359
00:13:05,955 --> 00:13:09,395
this works that's developed by

360
00:13:09,395 --> 00:13:10,615
reading all of the references

361
00:13:10,675 --> 00:13:12,215
at the bottom of this page,

362
00:13:12,275 --> 00:13:14,340
and and then our
subject matter

363
00:13:14,340 --> 00:13:16,760
experts basically
summarize those.

364
00:13:17,220 --> 00:13:18,900
And they also add a a section

365
00:13:18,900 --> 00:13:19,560
on consideration.

366
00:13:19,940 --> 00:13:23,135
So, in this case,
with this technique,

367
00:13:23,195 --> 00:13:24,555
they're talking
about potential

368
00:13:24,555 --> 00:13:26,495
false positives and
false negatives.

369
00:13:28,780 --> 00:13:33,100
Then, once we
specify what this

370
00:13:33,100 --> 00:13:34,880
technique is doing to what

371
00:13:35,100 --> 00:13:36,400
specific digital artifact,

372
00:13:36,540 --> 00:13:38,785
we can then infer
what are the related

373
00:13:38,925 --> 00:13:40,125
offensive techniques that we

374
00:13:40,125 --> 00:13:41,025
might care about.

375
00:13:41,405 --> 00:13:42,525
Now this,

376
00:13:43,245 --> 00:13:45,725
this table down
here is sort of

377
00:13:45,725 --> 00:13:49,170
a a miniature, attack table.

378
00:13:49,170 --> 00:13:51,890
So if you go to
attack dot mitre

379
00:13:51,890 --> 00:13:54,150
dot org, you'll see
the full table,

380
00:13:54,450 --> 00:13:56,210
which has, around five hundred

381
00:13:56,210 --> 00:13:57,110
of these techniques.

382
00:13:57,365 --> 00:13:58,725
In this case, we're selecting

383
00:13:58,725 --> 00:14:02,405
only the relevant ones
for the user to,

384
00:14:02,725 --> 00:14:04,505
review and determine, okay.

385
00:14:05,140 --> 00:14:06,680
Will this defensive
technique,

386
00:14:07,060 --> 00:14:08,120
protect me against,

387
00:14:08,900 --> 00:14:10,200
these offensive techniques?

388
00:14:12,195 --> 00:14:14,135
And then
the semantic relationships

389
00:14:14,195 --> 00:14:16,515
go both ways. So some of how

390
00:14:16,515 --> 00:14:17,655
we're doing the inference,

391
00:14:18,275 --> 00:14:22,380
takes into account,
hierarchy and,

392
00:14:22,840 --> 00:14:25,240
the type hierarchy
within the the

393
00:14:25,240 --> 00:14:26,380
artifact definition.

394
00:14:26,520 --> 00:14:29,720
So if a higher
order concept is

395
00:14:29,720 --> 00:14:32,125
interacting with
a digital artifact,

396
00:14:32,665 --> 00:14:34,585
then that flows
down to the lower

397
00:14:34,585 --> 00:14:36,205
order concepts as well.

398
00:14:36,585 --> 00:14:37,545
So in this case,

399
00:14:37,545 --> 00:14:39,390
we're showing if if
you hover here,

400
00:14:39,390 --> 00:14:40,670
you can see the relationships

401
00:14:40,670 --> 00:14:42,210
on the offensive side.

402
00:14:43,230 --> 00:14:45,570
Process hollowing is modifying

403
00:14:45,630 --> 00:14:46,830
a process code segment,

404
00:14:46,830 --> 00:14:47,745
but in this case,

405
00:14:47,745 --> 00:14:49,345
credential API hooking might

406
00:14:49,345 --> 00:14:52,065
modify or may modify a process

407
00:14:52,065 --> 00:14:52,805
code segment.

408
00:14:53,265 --> 00:14:55,365
So a lot of times
in cybersecurity,

409
00:14:56,110 --> 00:14:58,510
you're dealing
with maybes, and,

410
00:14:59,390 --> 00:15:01,170
we've at least come
up with a systematic

411
00:15:01,310 --> 00:15:03,550
way to describe those sorts of

412
00:15:03,550 --> 00:15:05,895
scenarios. And then down here

413
00:15:05,895 --> 00:15:08,135
is the references we used to

414
00:15:08,135 --> 00:15:09,355
develop this article,

415
00:15:09,815 --> 00:15:11,895
and you can go click those and

416
00:15:11,895 --> 00:15:12,935
see the in this case,

417
00:15:12,935 --> 00:15:14,875
this is a particular
patent that

418
00:15:15,120 --> 00:15:16,900
it links to, and you can see,

419
00:15:17,360 --> 00:15:19,840
what we read to
basically develop

420
00:15:19,840 --> 00:15:21,300
this article. Right?

421
00:15:21,600 --> 00:15:22,800
There's a number
of other things

422
00:15:22,800 --> 00:15:24,580
you can do with
the user interface.

423
00:15:24,885 --> 00:15:27,225
You can look up topics by

424
00:15:27,285 --> 00:15:29,385
offensive technique
or defensive

425
00:15:29,525 --> 00:15:31,385
technique with these
two search boxes.

426
00:15:31,980 --> 00:15:33,580
So if I wanna look
up a defensive

427
00:15:33,580 --> 00:15:35,260
technique like
dynamic analysis,

428
00:15:35,260 --> 00:15:36,480
I can do that here,

429
00:15:36,620 --> 00:15:38,400
and that'll take
me to that page.

430
00:15:39,020 --> 00:15:40,720
Or I can look up the offensive

431
00:15:40,780 --> 00:15:42,335
technique. Let's say I wanna

432
00:15:42,335 --> 00:15:43,235
look up rootkit.

433
00:15:44,415 --> 00:15:46,095
What do what do attackers do

434
00:15:46,095 --> 00:15:47,635
when they install a rootkit?

435
00:15:47,935 --> 00:15:49,555
Well, if you look
at this graph,

436
00:15:49,910 --> 00:15:51,610
we sort of codify that,

437
00:15:52,870 --> 00:15:54,870
and the offensive technique is

438
00:15:54,870 --> 00:15:55,610
in the middle.

439
00:15:55,990 --> 00:15:58,525
Some inferred
defensive techniques

440
00:15:58,525 --> 00:15:59,405
are on the left,

441
00:15:59,405 --> 00:16:01,725
and then the relevant digital

442
00:16:01,725 --> 00:16:03,325
artifacts are on
the right here.

443
00:16:03,325 --> 00:16:04,445
And we're showing that this

444
00:16:04,445 --> 00:16:06,225
offensive technique
might modify

445
00:16:06,910 --> 00:16:09,170
kernel modules,
shared library files,

446
00:16:10,270 --> 00:16:11,410
the kernel itself,

447
00:16:11,870 --> 00:16:13,730
or firmware on the system.

448
00:16:14,165 --> 00:16:18,105
So, then we infer
because these

449
00:16:18,245 --> 00:16:20,025
defensive artifacts
are interacting

450
00:16:20,085 --> 00:16:22,485
with those that
they might detect

451
00:16:22,485 --> 00:16:24,730
it or you could deceive this

452
00:16:24,730 --> 00:16:26,410
offensive technique
or you might

453
00:16:26,410 --> 00:16:28,090
harden against this
offensive technique.

454
00:16:28,090 --> 00:16:30,810
Right? And then you can click

455
00:16:30,810 --> 00:16:33,475
on any of these boxes and get

456
00:16:33,475 --> 00:16:36,835
definitions. So what is a what

457
00:16:36,835 --> 00:16:38,355
is firmware in defend?

458
00:16:38,355 --> 00:16:39,575
Well, if you click that,

459
00:16:39,635 --> 00:16:41,095
it takes you to this page,

460
00:16:41,670 --> 00:16:44,650
which has a definition and

461
00:16:44,710 --> 00:16:45,850
additional relationships.

462
00:16:46,550 --> 00:16:48,070
In this case, it's showing you

463
00:16:48,070 --> 00:16:50,055
firmware verification, might,

464
00:16:50,455 --> 00:16:51,355
verify firmware.

465
00:16:52,215 --> 00:16:52,935
And then there's,

466
00:16:53,255 --> 00:16:55,255
some specific
offensive techniques

467
00:16:55,255 --> 00:16:57,515
that attackers do to
modify firmware.

468
00:16:58,410 --> 00:16:59,530
And in this case,

469
00:16:59,530 --> 00:17:00,650
we're showing
different types.

470
00:17:00,650 --> 00:17:03,130
So you see, there's firmware,

471
00:17:03,130 --> 00:17:04,270
but then there's specifically

472
00:17:04,490 --> 00:17:05,975
system firmware as well.

473
00:17:06,935 --> 00:17:08,635
So some offensive techniques

474
00:17:08,695 --> 00:17:11,035
modify your system firmware,

475
00:17:11,335 --> 00:17:12,715
and some of them
might modify,

476
00:17:13,255 --> 00:17:15,035
peripheral firmware,
for example.

477
00:17:15,300 --> 00:17:16,660
So that's a quick spin through

478
00:17:16,660 --> 00:17:17,720
the user interface.

479
00:17:20,660 --> 00:17:22,100
So I wanna talk a little bit

480
00:17:22,100 --> 00:17:23,640
about how we name
these things.

481
00:17:24,055 --> 00:17:25,495
Basically, the methodology I

482
00:17:25,495 --> 00:17:27,735
talked about earlier was is

483
00:17:27,735 --> 00:17:29,495
pretty straightforward,
but it,

484
00:17:29,815 --> 00:17:31,335
kind of embarrassingly took us

485
00:17:31,335 --> 00:17:32,955
a long time to
figure this out.

486
00:17:33,260 --> 00:17:35,260
We simply describe
it along two

487
00:17:35,260 --> 00:17:36,480
dimensions of specificity,

488
00:17:37,340 --> 00:17:39,680
the noun and the the verb.

489
00:17:40,220 --> 00:17:43,505
So we we say
the the digital artifact,

490
00:17:43,645 --> 00:17:45,965
the noun, or
defensive action,

491
00:17:45,965 --> 00:17:47,005
the verb. So,

492
00:17:48,020 --> 00:17:49,720
there's three examples here.

493
00:17:49,860 --> 00:17:51,460
The least specific
example would

494
00:17:51,460 --> 00:17:53,080
be segment analysis.

495
00:17:53,300 --> 00:17:55,480
And then as you move
up to the right,

496
00:17:55,780 --> 00:17:56,405
we get,

497
00:17:57,205 --> 00:17:58,805
the most specific process code

498
00:17:58,805 --> 00:17:59,685
segment verification,

499
00:17:59,685 --> 00:18:01,065
the example I gave earlier.

500
00:18:01,525 --> 00:18:04,265
Verification in this case here

501
00:18:04,405 --> 00:18:06,760
is we're saying is
a more specific

502
00:18:07,140 --> 00:18:08,120
type of analysis.

503
00:18:09,620 --> 00:18:11,140
And the reason we took this

504
00:18:11,140 --> 00:18:14,200
approach was we
wanted multiple

505
00:18:15,115 --> 00:18:16,555
teams within
an organization to

506
00:18:16,555 --> 00:18:19,535
be able to use
the same, framework.

507
00:18:19,995 --> 00:18:21,275
If you have someone working in

508
00:18:21,275 --> 00:18:24,250
acquisitions,
they may not care

509
00:18:24,250 --> 00:18:26,350
too much about the specific

510
00:18:26,730 --> 00:18:29,930
the most detailed
example of, let's say,

511
00:18:29,930 --> 00:18:32,075
file analysis. They just wanna

512
00:18:32,075 --> 00:18:33,515
track whether or not they have

513
00:18:33,515 --> 00:18:35,835
a file analysis capability or

514
00:18:35,835 --> 00:18:37,295
a process analysis
capability.

515
00:18:37,355 --> 00:18:38,655
But if you have
a practitioner,

516
00:18:39,490 --> 00:18:40,930
they might want to know about

517
00:18:40,930 --> 00:18:42,210
all the different
ways you could

518
00:18:42,210 --> 00:18:45,030
do file analysis or
process analysis.

519
00:18:45,650 --> 00:18:49,535
And by having that
taxonomical approach,

520
00:18:49,915 --> 00:18:51,915
it lets us address
both technical

521
00:18:51,915 --> 00:18:53,675
and nontechnical
users and then

522
00:18:53,675 --> 00:18:56,315
capture metrics
accordingly with

523
00:18:56,315 --> 00:18:58,770
the same language
framework. Right?

524
00:19:00,750 --> 00:19:02,590
So I'll talk a little bit here

525
00:19:02,590 --> 00:19:04,690
about how people
are using defend

526
00:19:05,155 --> 00:19:08,515
to answer questions
about their

527
00:19:08,515 --> 00:19:09,815
cybersecurity posture.

528
00:19:10,435 --> 00:19:11,575
So in this example,

529
00:19:12,115 --> 00:19:13,815
we took a specific threat.

530
00:19:15,040 --> 00:19:17,440
It was, identified
in this threat

531
00:19:17,440 --> 00:19:19,700
report. You can see
the link there.

532
00:19:20,640 --> 00:19:22,660
And then they wanted to know,

533
00:19:23,365 --> 00:19:25,705
would we have had
any protections

534
00:19:25,845 --> 00:19:28,005
against this
threat given these

535
00:19:28,005 --> 00:19:29,765
three specific
products that we

536
00:19:29,765 --> 00:19:32,740
purchased? So what we did was

537
00:19:32,740 --> 00:19:34,680
we took the threat report,

538
00:19:34,740 --> 00:19:36,820
and it was mapped to MITRE ATT

539
00:19:36,820 --> 00:19:38,680
and CK in terms
of the offensive

540
00:19:39,425 --> 00:19:41,605
techniques that
the adversary was using.

541
00:19:42,785 --> 00:19:45,685
So you can see in this attack

542
00:19:46,065 --> 00:19:47,845
matrix view on the left,

543
00:19:48,830 --> 00:19:50,130
for initial access,

544
00:19:50,670 --> 00:19:53,890
they executed a supply
chain compromise.

545
00:19:54,910 --> 00:19:55,810
For persistence,

546
00:19:56,030 --> 00:19:57,915
they installed
a scheduled task

547
00:19:57,915 --> 00:20:01,275
job and a boot auto start

548
00:20:01,275 --> 00:20:02,335
execution script.

549
00:20:03,675 --> 00:20:06,380
And then what we
did was we took

550
00:20:07,000 --> 00:20:08,520
the defensive
products and broke

551
00:20:08,520 --> 00:20:11,420
them down using defend and

552
00:20:11,480 --> 00:20:13,260
identified what
they were doing.

553
00:20:14,515 --> 00:20:15,875
So of these three products,

554
00:20:15,875 --> 00:20:17,175
they were doing some hardening

555
00:20:17,235 --> 00:20:19,415
techniques, some
detection techniques,

556
00:20:19,475 --> 00:20:21,175
and some isolation
techniques.

557
00:20:22,620 --> 00:20:41,610
And then But

558
00:20:41,610 --> 00:20:43,070
they were not part of these

559
00:20:43,425 --> 00:20:44,725
three products' capabilities,

560
00:20:45,105 --> 00:20:47,205
so that's marked as
red and purple.

561
00:20:47,665 --> 00:20:49,025
And then we step
through the rest

562
00:20:49,025 --> 00:20:49,845
of the attack,

563
00:20:50,500 --> 00:20:53,140
and we had some
capability in this area.

564
00:20:53,140 --> 00:20:55,140
So that was marked amber and

565
00:20:55,140 --> 00:20:58,280
then red in this other area.

566
00:20:58,595 --> 00:21:00,035
So the result was that you get

567
00:21:00,035 --> 00:21:01,235
these sort of higher level

568
00:21:01,235 --> 00:21:02,615
insights about,

569
00:21:04,035 --> 00:21:05,395
where you might
wanna make your

570
00:21:05,395 --> 00:21:08,040
investments. You might wanna,

571
00:21:08,040 --> 00:21:09,480
in this case, make
your investments

572
00:21:09,480 --> 00:21:10,940
more in application
hardening,

573
00:21:11,000 --> 00:21:11,720
platform hardening,

574
00:21:11,720 --> 00:21:12,920
and operating
system monitoring

575
00:21:12,920 --> 00:21:14,540
because you don't
have anything there.

576
00:21:15,295 --> 00:21:18,195
Now they did have some
capability here,

577
00:21:18,415 --> 00:21:20,355
but here's a case where,

578
00:21:21,055 --> 00:21:22,515
everybody's sort
of different.

579
00:21:22,790 --> 00:21:24,230
If you're in a well resourced

580
00:21:24,230 --> 00:21:25,750
environment and you wanna add

581
00:21:25,750 --> 00:21:27,690
diversity and depth
to your defenses,

582
00:21:28,070 --> 00:21:29,610
you might actually
make additional

583
00:21:29,670 --> 00:21:31,050
investments in this area,

584
00:21:32,525 --> 00:21:34,865
to add diversity
to your defenses,

585
00:21:36,285 --> 00:21:38,045
or double up on certain types

586
00:21:38,045 --> 00:21:39,665
of analytics, for example.

587
00:21:40,390 --> 00:21:41,590
But if you're in a resource

588
00:21:41,590 --> 00:21:43,770
constrained environment,
you might,

589
00:21:44,950 --> 00:21:46,470
take some of
the investment you

590
00:21:46,470 --> 00:21:48,230
put in this area
and put it into

591
00:21:48,230 --> 00:21:50,235
the areas where you
had no capability.

592
00:21:50,935 --> 00:21:52,455
So the idea is that depending

593
00:21:52,455 --> 00:21:53,655
on what question you're trying

594
00:21:53,655 --> 00:21:55,415
to answer, you
can use the same

595
00:21:55,415 --> 00:21:58,340
framework to tell
the story about,

596
00:21:59,040 --> 00:22:00,320
what direction you think your

597
00:22:00,320 --> 00:22:01,700
organization should go.

598
00:22:06,455 --> 00:22:08,215
So finally, I'll
leave you with this.

599
00:22:08,215 --> 00:22:09,815
This is a project that we open

600
00:22:09,815 --> 00:22:12,135
source to collaborate
with the industry

601
00:22:12,135 --> 00:22:14,990
on. So I'll close out with,

602
00:22:15,710 --> 00:22:18,110
how to get involved
with the project

603
00:22:18,110 --> 00:22:19,650
if you're interested in it.

604
00:22:20,670 --> 00:22:21,890
You can ask vendors,

605
00:22:22,865 --> 00:22:24,385
what defend techniques they

606
00:22:24,385 --> 00:22:26,165
support when you're
making an acquisition

607
00:22:26,225 --> 00:22:27,125
in this space.

608
00:22:28,305 --> 00:22:29,985
And by thinking in terms of

609
00:22:29,985 --> 00:22:32,740
digital artifacts and
defensive verbs,

610
00:22:32,740 --> 00:22:35,080
you can start to
answer this question.

611
00:22:36,500 --> 00:22:38,600
With this specific
defend technique,

612
00:22:39,025 --> 00:22:41,345
we can detect these
attack techniques.

613
00:22:41,345 --> 00:22:42,625
And then that tells the whole

614
00:22:42,625 --> 00:22:45,185
story about what
specific function

615
00:22:45,185 --> 00:22:48,750
in your enterprise is is is

616
00:22:48,750 --> 00:22:50,530
dealing with
a particular threat.

617
00:22:50,670 --> 00:22:52,590
Right? The knowledge
base, again,

618
00:22:52,590 --> 00:22:54,670
is available at
defend dot mitre

619
00:22:54,670 --> 00:22:56,715
dot org, and we've published

620
00:22:56,715 --> 00:22:59,535
the ontology on
that page as well.

621
00:22:59,995 --> 00:23:02,735
And it's, it's
an OWL two format,

622
00:23:03,670 --> 00:23:05,190
and people can download it and

623
00:23:05,190 --> 00:23:07,210
extend it as they see fit.

624
00:23:07,510 --> 00:23:09,210
And we're looking
for contributions.

625
00:23:09,350 --> 00:23:11,530
So if folks wanna add to it,

626
00:23:12,295 --> 00:23:14,535
they can send us an email at

627
00:23:14,535 --> 00:23:16,235
defend at mitre dot org,

628
00:23:16,295 --> 00:23:18,375
and we'll be very
happy to engage

629
00:23:18,375 --> 00:23:19,035
with you.

630
00:23:20,160 --> 00:23:21,600
So thank you for
the opportunity

631
00:23:21,600 --> 00:23:23,860
to speak here to
you all, and,

632
00:23:25,360 --> 00:23:26,740
I look forward to
your questions.

