1
00:00:04,540 --> 00:00:05,820
This presentation will discuss

2
00:00:05,820 --> 00:00:07,420
the opportunities
for leveraging

3
00:00:07,420 --> 00:00:09,420
Graphcore's IPU
architecture for

4
00:00:09,420 --> 00:00:10,560
graph neural networks.

5
00:00:12,205 --> 00:00:13,665
We will review
the computational

6
00:00:13,725 --> 00:00:15,085
requirements and challenges of

7
00:00:15,085 --> 00:00:17,645
graph networks
that require both

8
00:00:17,645 --> 00:00:18,925
sparse access to memory,

9
00:00:18,925 --> 00:00:19,825
a sparse computation,

10
00:00:20,430 --> 00:00:21,390
and dense processing,

11
00:00:21,390 --> 00:00:22,670
which is associated
with the use

12
00:00:22,670 --> 00:00:24,430
of neural networks as function

13
00:00:24,430 --> 00:00:26,830
approximators. We will then

14
00:00:26,830 --> 00:00:28,130
introduce the intelligent

15
00:00:28,190 --> 00:00:29,650
processing unit, the IPU,

16
00:00:30,575 --> 00:00:32,275
and review the design
principles,

17
00:00:32,655 --> 00:00:34,415
the processor architecture and

18
00:00:34,415 --> 00:00:35,555
the system scale out.

19
00:00:36,415 --> 00:00:37,695
Finally we will discuss

20
00:00:37,695 --> 00:00:39,500
implementation
opportunities for

21
00:00:39,500 --> 00:00:41,360
graph neural networks
on the IPO.

22
00:00:43,580 --> 00:00:45,040
We will start by considering

23
00:00:45,180 --> 00:00:46,620
graph neural
network processing

24
00:00:46,620 --> 00:00:47,600
and its challenges.

25
00:00:50,235 --> 00:00:51,675
Graph neural
networks have been

26
00:00:51,675 --> 00:00:53,755
successfully applied
to a growing

27
00:00:53,755 --> 00:00:55,275
number of
applications based on

28
00:00:55,275 --> 00:00:56,495
graph structured data.

29
00:00:57,210 --> 00:00:58,490
Applications that range from

30
00:00:58,490 --> 00:01:00,030
the small graphs of molecular

31
00:01:00,170 --> 00:01:02,330
chemistry and
drug discovery to

32
00:01:02,330 --> 00:01:03,950
the very large graphs
of recommended

33
00:01:04,010 --> 00:01:07,475
systems. This
application involved

34
00:01:07,535 --> 00:01:09,635
tasks including node
classification,

35
00:01:10,335 --> 00:01:11,155
link prediction,

36
00:01:11,215 --> 00:01:12,275
and graph classification.

37
00:01:16,810 --> 00:01:18,250
However, graph neural networks

38
00:01:18,250 --> 00:01:19,710
come with a number
of challenges.

39
00:01:20,395 --> 00:01:21,835
The first challenge is related

40
00:01:21,835 --> 00:01:22,975
to sparse computation.

41
00:01:24,315 --> 00:01:25,835
The implementation
of the graph

42
00:01:25,835 --> 00:01:28,075
neural network is
based on the message

43
00:01:28,075 --> 00:01:28,975
passing algorithm,

44
00:01:29,770 --> 00:01:31,610
which is implemented
as a series

45
00:01:31,610 --> 00:01:34,110
of message passing
steps or GNN layers.

46
00:01:34,970 --> 00:01:36,330
And for each
individual message

47
00:01:36,330 --> 00:01:38,415
passing step, the embedding of

48
00:01:38,415 --> 00:01:40,975
a node is updated
using the neighbors

49
00:01:40,975 --> 00:01:42,975
of the node. And for each of

50
00:01:42,975 --> 00:01:44,115
the neighboring nodes,

51
00:01:44,630 --> 00:01:46,310
the computation
involves the use

52
00:01:46,310 --> 00:01:48,710
of a message
function to compute

53
00:01:48,710 --> 00:01:50,470
a message associated
with the neighboring

54
00:01:50,470 --> 00:01:52,470
node, and all the neighboring

55
00:01:52,470 --> 00:01:54,090
node messages are
then aggregated

56
00:01:54,775 --> 00:01:56,715
and sent to
an update function,

57
00:01:57,495 --> 00:02:00,235
that produces the update,

58
00:02:00,295 --> 00:02:01,895
the sequence to
update the embedding

59
00:02:01,895 --> 00:02:04,110
of the node. And
both the message

60
00:02:04,110 --> 00:02:05,470
function and
the update function

61
00:02:05,470 --> 00:02:06,750
are parameterized by neural

62
00:02:06,750 --> 00:02:08,670
networks and
therefore corresponds

63
00:02:08,670 --> 00:02:09,890
to dense computation,

64
00:02:10,875 --> 00:02:12,235
where there is
at the same time

65
00:02:12,235 --> 00:02:13,615
irregularly
sparse connectivity

66
00:02:14,155 --> 00:02:15,755
associated with
gathering the neighbours

67
00:02:15,755 --> 00:02:17,215
of each individual node.

68
00:02:17,850 --> 00:02:19,390
This GNN sparse connectivity

69
00:02:19,850 --> 00:02:21,310
results in lower arithmetic

70
00:02:21,370 --> 00:02:23,150
intensity and
higher communication

71
00:02:23,370 --> 00:02:25,630
costs compared to
dense processing.

72
00:02:26,225 --> 00:02:27,505
And these computations are

73
00:02:27,505 --> 00:02:29,665
particularly challenging
for modern,

74
00:02:29,985 --> 00:02:30,965
hardware accelerators.

75
00:02:33,960 --> 00:02:35,560
Another challenge
is related to

76
00:02:35,560 --> 00:02:36,780
heterogeneous graphs,

77
00:02:37,080 --> 00:02:38,840
which are graphs with multiple

78
00:02:38,840 --> 00:02:41,180
node types and multiple edge

79
00:02:41,240 --> 00:02:42,380
types or relations.

80
00:02:43,865 --> 00:02:44,825
For these graphs,

81
00:02:44,825 --> 00:02:46,585
for each of the relation types

82
00:02:46,585 --> 00:02:47,705
there is a different set of

83
00:02:47,705 --> 00:02:48,845
neural network parameters,

84
00:02:49,625 --> 00:02:51,225
in addition to
having different

85
00:02:51,225 --> 00:02:52,605
set of neural
network parameters

86
00:02:52,810 --> 00:02:55,930
for the individual
layers of the g n n,

87
00:02:55,930 --> 00:02:57,450
for individual message passing

88
00:02:57,450 --> 00:02:59,470
steps of the g n
n computation.

89
00:03:00,735 --> 00:03:03,395
This, considering
that in general

90
00:03:03,455 --> 00:03:05,215
in the order of thousands of

91
00:03:05,215 --> 00:03:07,555
relation types for
this type of graphs,

92
00:03:07,855 --> 00:03:09,395
this corresponds
to a significant

93
00:03:09,535 --> 00:03:11,090
increase of the number of

94
00:03:11,090 --> 00:03:12,230
parameters to train,

95
00:03:12,690 --> 00:03:13,990
which can cause overfitting.

96
00:03:17,490 --> 00:03:18,690
Another challenge of graph

97
00:03:18,690 --> 00:03:20,635
networks is related
to the fact

98
00:03:20,635 --> 00:03:21,755
that increasing the number of

99
00:03:21,755 --> 00:03:24,235
GNN layers, and each each one

100
00:03:24,235 --> 00:03:25,035
of these layers,

101
00:03:25,435 --> 00:03:26,815
corresponded to an aggregation

102
00:03:27,275 --> 00:03:28,255
of neighbors,

103
00:03:29,850 --> 00:03:31,310
this causes an exponential

104
00:03:31,370 --> 00:03:32,990
increase of multi
hop neighbors,

105
00:03:33,370 --> 00:03:34,970
which causes an increase of

106
00:03:34,970 --> 00:03:36,570
computational complexity and

107
00:03:36,570 --> 00:03:37,550
memory cost.

108
00:03:38,495 --> 00:03:40,255
At the same time increasing

109
00:03:40,255 --> 00:03:41,955
the number of
neighbors aggregated

110
00:03:42,335 --> 00:03:45,235
over several hops
and aggregating

111
00:03:45,455 --> 00:03:47,010
information that
corresponds to

112
00:03:47,410 --> 00:03:49,190
an increasing number of nodes

113
00:03:49,570 --> 00:03:50,950
that are part of the graphs,

114
00:03:51,250 --> 00:03:52,530
makes the representation of

115
00:03:52,530 --> 00:03:53,910
the nodes more similar,

116
00:03:54,130 --> 00:03:55,670
which causes over smoothing.

117
00:03:56,765 --> 00:03:58,365
Oversmoothing can
correspond to

118
00:03:58,365 --> 00:03:59,905
a degradation of
the task performance.

119
00:04:02,045 --> 00:04:03,825
Another challenge
is related to

120
00:04:03,965 --> 00:04:05,025
dynamic graphs.

121
00:04:05,700 --> 00:04:07,540
Dynamic graphs
are prevalent in

122
00:04:07,540 --> 00:04:09,960
real life systems
with applications

123
00:04:10,260 --> 00:04:11,640
related to social networks,

124
00:04:12,020 --> 00:04:14,600
forecasting,
epidemiology and others.

125
00:04:15,885 --> 00:04:18,225
And these types
of dynamic graphs

126
00:04:18,605 --> 00:04:21,085
evolve over time features of

127
00:04:21,085 --> 00:04:21,745
their connectivity,

128
00:04:22,205 --> 00:04:24,280
have for example additions or

129
00:04:24,280 --> 00:04:26,300
deletions of nodes or edges,

130
00:04:26,920 --> 00:04:28,600
and have the transformation of

131
00:04:28,600 --> 00:04:30,940
the features of
nodes and edges.

132
00:04:32,335 --> 00:04:34,095
An issue related to this type

133
00:04:34,095 --> 00:04:36,035
of graphs is that
when collecting

134
00:04:36,335 --> 00:04:38,015
a number of examples in a mini

135
00:04:38,015 --> 00:04:39,155
batch for training,

136
00:04:39,855 --> 00:04:42,350
especially when
the there is a large

137
00:04:42,570 --> 00:04:43,390
mini batch,

138
00:04:44,410 --> 00:04:46,170
the information
that is evolving

139
00:04:46,170 --> 00:04:48,970
over time related to the last

140
00:04:48,970 --> 00:04:50,830
samples in the batch lacks

141
00:04:51,915 --> 00:04:53,275
inaccurate up to
date information

142
00:04:53,275 --> 00:04:55,275
for the early examples
in the batch.

143
00:04:55,275 --> 00:04:56,955
So there is a staleness not up

144
00:04:56,955 --> 00:04:58,610
to date information that again

145
00:04:58,610 --> 00:05:00,290
can correspond
to a degradation

146
00:05:00,290 --> 00:05:01,190
of task performance.

147
00:05:03,010 --> 00:05:05,170
A final issue
related to training

148
00:05:05,170 --> 00:05:06,825
of graph neural
networks relates

149
00:05:06,825 --> 00:05:08,285
to large scale deep learning.

150
00:05:08,985 --> 00:05:11,145
We have already
described the issues

151
00:05:11,145 --> 00:05:12,605
of deeper GNNs,

152
00:05:13,305 --> 00:05:14,920
graph neural
networks that have

153
00:05:15,480 --> 00:05:17,080
a large number of
message passing

154
00:05:17,080 --> 00:05:19,080
steps which can
cause a neighborhood

155
00:05:19,080 --> 00:05:20,460
explosion and oversmoothing.

156
00:05:21,560 --> 00:05:22,600
Besides those problems when

157
00:05:22,600 --> 00:05:23,945
training very large graphs,

158
00:05:24,105 --> 00:05:26,425
there is the issue
that training

159
00:05:26,425 --> 00:05:27,865
still requires efficient and

160
00:05:27,865 --> 00:05:29,325
fine grained
access to memory.

161
00:05:30,185 --> 00:05:31,465
And given the high level of

162
00:05:31,465 --> 00:05:33,290
sparsity of large graphs,

163
00:05:33,910 --> 00:05:35,750
when accessing a memory it is

164
00:05:35,750 --> 00:05:36,870
possible that only a fraction

165
00:05:36,870 --> 00:05:38,330
of the communication bandwidth

166
00:05:38,470 --> 00:05:40,330
may correspond to
useful bandwidth.

167
00:05:43,005 --> 00:05:44,125
We will now review the main

168
00:05:44,125 --> 00:05:45,565
features of the intelligence

169
00:05:45,565 --> 00:05:46,865
processing unit architecture.

170
00:05:48,605 --> 00:05:50,385
The intelligence
processing unit

171
00:05:50,580 --> 00:05:52,180
has been designed
from the ground

172
00:05:52,180 --> 00:05:54,120
up to accelerate AI workloads

173
00:05:54,260 --> 00:05:55,960
and to achieve
the following goals:

174
00:05:56,820 --> 00:05:59,160
Fast and fixed cost
access to memory,

175
00:06:00,035 --> 00:06:01,475
Efficient fine
grained computation

176
00:06:01,475 --> 00:06:02,455
with low latency.

177
00:06:03,475 --> 00:06:04,695
The possibility of efficiently

178
00:06:04,835 --> 00:06:06,855
scaling to systems
that include

179
00:06:07,075 --> 00:06:08,755
a significantly high number of

180
00:06:08,755 --> 00:06:11,240
processors. And finally,

181
00:06:11,540 --> 00:06:12,900
the possibility of maintaining

182
00:06:12,900 --> 00:06:14,660
high performance
for low precision

183
00:06:14,660 --> 00:06:16,820
compute, when using
low precision

184
00:06:16,820 --> 00:06:19,395
number formats
that deliver high

185
00:06:19,395 --> 00:06:20,535
computational efficiency,

186
00:06:20,915 --> 00:06:22,215
for low power consumption,

187
00:06:22,515 --> 00:06:24,055
and reduced memory cost.

188
00:06:26,970 --> 00:06:28,490
This has resulted
in the following

189
00:06:28,490 --> 00:06:30,590
main characteristics
of the IPU process:

190
00:06:31,530 --> 00:06:33,150
A very large number
of independent

191
00:06:33,210 --> 00:06:35,785
cores, and distributed
on chip SRAM.

192
00:06:36,885 --> 00:06:38,725
Computation that is based on

193
00:06:38,725 --> 00:06:40,085
multiple instruction multiple

194
00:06:40,085 --> 00:06:42,740
data parallelism and relies on

195
00:06:42,740 --> 00:06:44,020
a bulk synchronous parallel

196
00:06:44,020 --> 00:06:45,160
programming model,

197
00:06:45,700 --> 00:06:47,540
which allows to exchange data

198
00:06:47,540 --> 00:06:49,540
between different
cores of the processor

199
00:06:49,540 --> 00:06:50,760
without memory concurrencies.

200
00:06:52,165 --> 00:06:53,765
And finally, in addition to

201
00:06:53,765 --> 00:06:56,505
the provision delivered
by the hardware

202
00:06:56,645 --> 00:06:58,165
of efficient computation for

203
00:06:58,165 --> 00:07:00,490
fine grained
computational elements.

204
00:07:00,710 --> 00:07:02,970
This is also supported
by the software

205
00:07:03,670 --> 00:07:04,870
with the provision of sparse

206
00:07:04,870 --> 00:07:06,170
instructions and libraries.

207
00:07:09,535 --> 00:07:10,895
These slides give additional

208
00:07:10,895 --> 00:07:14,035
details on the IPU
processor, the GC200.

209
00:07:15,600 --> 00:07:17,360
A single processor
contains one

210
00:07:17,360 --> 00:07:18,800
thousand four
hundred and seventy

211
00:07:18,800 --> 00:07:20,900
two independent IPU tiles,

212
00:07:21,280 --> 00:07:23,220
we distributed in
processor memory.

213
00:07:23,595 --> 00:07:25,595
Each tile consists in other

214
00:07:25,595 --> 00:07:28,335
words of both
compute and memory,

215
00:07:28,715 --> 00:07:30,895
for a total of two
fifty teraflops

216
00:07:30,955 --> 00:07:32,655
of fourteen point
six in compute

217
00:07:32,950 --> 00:07:34,550
and one hundred
megabytes of in

218
00:07:34,550 --> 00:07:35,530
processor memory.

219
00:07:36,470 --> 00:07:38,250
The on chip memory
can be accessed

220
00:07:38,470 --> 00:07:39,990
with a bandwidth
of forty seven

221
00:07:39,990 --> 00:07:42,010
point five terabyte
per second.

222
00:07:45,785 --> 00:07:47,785
Four IPU processors can be put

223
00:07:47,785 --> 00:07:49,565
together to build an IPU M2000

224
00:07:49,705 --> 00:07:52,490
machine which deliver a total

225
00:07:52,490 --> 00:07:54,090
of one petaflops of floating

226
00:07:54,090 --> 00:07:56,250
point six in
compute and a total

227
00:07:56,250 --> 00:07:57,530
of three point
six gigabytes of

228
00:07:57,530 --> 00:07:58,670
in processor memory.

229
00:07:59,815 --> 00:08:01,915
The M2000 machine
also contains

230
00:08:02,295 --> 00:08:04,875
an IPU gateway and
external DRAM,

231
00:08:05,255 --> 00:08:07,095
for a total of
four forty eight

232
00:08:07,095 --> 00:08:08,635
gigabytes of
streaming memory.

233
00:08:10,570 --> 00:08:11,930
The connection
between the four

234
00:08:11,930 --> 00:08:13,710
processors, the four
IPU processors

235
00:08:14,090 --> 00:08:16,810
of the N2000
machine is provided

236
00:08:16,810 --> 00:08:18,190
by FastIP Link,

237
00:08:18,465 --> 00:08:19,825
that have a bandwidth of sixty

238
00:08:19,825 --> 00:08:21,125
four gigabyte per second.

239
00:08:24,865 --> 00:08:26,385
This slide compares the memory

240
00:08:26,385 --> 00:08:28,830
subsystems of
the F2000 machine

241
00:08:29,130 --> 00:08:30,970
with the memory subsystem of

242
00:08:30,970 --> 00:08:32,510
alternative hardware
accelerators.

243
00:08:33,370 --> 00:08:35,915
And the LPUM2000 have
a larger amount,

244
00:08:35,915 --> 00:08:37,195
a significantly larger amount

245
00:08:37,195 --> 00:08:38,255
of on chip memory,

246
00:08:38,435 --> 00:08:40,815
which is accessed
with extremely

247
00:08:40,955 --> 00:08:43,115
high memory
bandwidth in excess

248
00:08:43,115 --> 00:08:44,895
of forty seven
terabytes per second,

249
00:08:45,590 --> 00:08:46,950
and also have a much larger

250
00:08:46,950 --> 00:08:49,190
amount of DRAM with
lower bandwidth,

251
00:08:49,190 --> 00:08:50,630
which is motivated by the fact

252
00:08:50,630 --> 00:08:53,210
that the large amount
of on chip RAM,

253
00:08:53,670 --> 00:08:56,525
on chip SRAM
generally corresponds

254
00:08:56,525 --> 00:08:58,285
to a reduced
requirement of DRAM

255
00:08:58,285 --> 00:08:58,785
traffic.

256
00:09:04,280 --> 00:09:05,880
This slide gives
typical examples

257
00:09:05,880 --> 00:09:07,320
of the use of external memory

258
00:09:07,320 --> 00:09:09,180
for training of
neural networks.

259
00:09:10,075 --> 00:09:11,755
There is on the top the simple

260
00:09:11,755 --> 00:09:13,595
case where the entire
model fits

261
00:09:13,595 --> 00:09:15,595
on in processor
memory and there

262
00:09:15,595 --> 00:09:16,955
is no need to access external

263
00:09:16,955 --> 00:09:18,095
memory for training.

264
00:09:19,070 --> 00:09:20,210
In other situations,

265
00:09:20,830 --> 00:09:22,530
it's possible
to advantageously

266
00:09:23,070 --> 00:09:24,750
store master weights
and optimized

267
00:09:24,750 --> 00:09:26,690
states in external memory,

268
00:09:27,685 --> 00:09:28,745
especially for situation

269
00:09:29,125 --> 00:09:30,505
corresponding to pipeline

270
00:09:30,565 --> 00:09:31,845
parallelism where,

271
00:09:32,165 --> 00:09:33,865
there is only infrequent need

272
00:09:34,005 --> 00:09:36,460
of the weight update
and accessing

273
00:09:36,460 --> 00:09:38,700
external memory to
use the optimizer

274
00:09:38,700 --> 00:09:40,240
state for updating
the parameters.

275
00:09:41,660 --> 00:09:43,180
And finally, there is the case

276
00:09:43,180 --> 00:09:45,915
where there is is
useful to have

277
00:09:45,915 --> 00:09:47,675
a more regular
access to external

278
00:09:47,675 --> 00:09:49,535
memory with phase execution

279
00:09:50,075 --> 00:09:51,595
using streaming memory and

280
00:09:51,595 --> 00:09:53,615
overlapping of
communication and IO.

281
00:09:54,510 --> 00:09:55,650
And in this case,

282
00:09:55,710 --> 00:09:57,070
the periodicity of accessing

283
00:09:57,070 --> 00:10:00,430
the external DDR
is larger given

284
00:10:00,430 --> 00:10:02,530
the provision of
a large amount

285
00:10:02,805 --> 00:10:07,465
of on chip SRAM.

286
00:10:08,725 --> 00:10:10,085
The IPM two thousand and seven

287
00:10:10,085 --> 00:10:11,330
machine that we have just

288
00:10:11,330 --> 00:10:12,770
described can be used to build

289
00:10:12,770 --> 00:10:13,910
even larger systems.

290
00:10:14,450 --> 00:10:17,490
The IPOPOD64 is constituted by

291
00:10:17,490 --> 00:10:19,510
sixteen IPOM2000 machines

292
00:10:19,825 --> 00:10:21,345
corresponding to
sixty four IPU

293
00:10:21,345 --> 00:10:23,585
processors, and
provides a total

294
00:10:23,585 --> 00:10:25,185
of sixteen petaflops
or fourteen

295
00:10:25,185 --> 00:10:26,325
point six in compute,

296
00:10:27,040 --> 00:10:28,320
corresponding to fifty seven

297
00:10:28,320 --> 00:10:29,760
point six gigabytes
of importer

298
00:10:29,760 --> 00:10:32,340
memory and around
seven terabyte

299
00:10:32,400 --> 00:10:33,460
of streaming memory.

300
00:10:34,615 --> 00:10:37,115
And these IPU pods can be also

301
00:10:37,255 --> 00:10:39,595
used to build even
larger systems,

302
00:10:40,215 --> 00:10:41,975
aggregating several racks,

303
00:10:41,975 --> 00:10:43,195
several IPU pods,

304
00:10:43,940 --> 00:10:46,500
to arrive to hundreds and even

305
00:10:46,500 --> 00:10:47,780
thousands of processors up to

306
00:10:47,780 --> 00:10:49,160
sixty four thousand
processors.

307
00:10:49,700 --> 00:10:51,620
And these very
large systems are

308
00:10:51,620 --> 00:10:53,400
glued together by
the IPU fabric,

309
00:10:53,975 --> 00:10:55,915
which provides
connections between

310
00:10:56,135 --> 00:10:57,675
the individual IPU processors

311
00:10:58,055 --> 00:11:01,355
of each M2000 machine
within an IPU pod,

312
00:11:01,895 --> 00:11:03,355
as you have described before,

313
00:11:04,380 --> 00:11:05,760
and a connection to the host,

314
00:11:06,060 --> 00:11:07,920
and connection
through the gateway

315
00:11:08,060 --> 00:11:09,660
link between different racks

316
00:11:09,660 --> 00:11:11,260
between different IPo pods of

317
00:11:11,260 --> 00:11:12,640
a much larger system.

318
00:11:16,045 --> 00:11:17,825
We'll now consider
the implementation

319
00:11:17,965 --> 00:11:19,985
opportunities
offered by the IPU.

320
00:11:23,320 --> 00:11:24,520
Implementation
opportunities for

321
00:11:24,520 --> 00:11:26,840
graph processing
on the IPO are

322
00:11:26,840 --> 00:11:28,840
given by the scalable and cost

323
00:11:28,840 --> 00:11:30,675
effective DRAM
that can be used

324
00:11:30,675 --> 00:11:31,895
to store large embedding,

325
00:11:32,835 --> 00:11:34,275
but the possible use of large

326
00:11:34,275 --> 00:11:35,795
SRAM to store neural network

327
00:11:35,795 --> 00:11:37,095
weights and model data,

328
00:11:38,250 --> 00:11:40,110
but the use of
distributed SRAM

329
00:11:40,410 --> 00:11:41,850
rely on the BSP programming

330
00:11:41,850 --> 00:11:44,010
model to deliver efficient

331
00:11:44,010 --> 00:11:45,470
execution of it
for heterogeneous

332
00:11:45,610 --> 00:11:48,915
computation and finally
the possibility

333
00:11:48,915 --> 00:11:50,675
of using the fast
APU links for

334
00:11:50,675 --> 00:11:52,455
efficient computation
in pipelines

335
00:11:52,915 --> 00:11:54,215
or for phased execution.

336
00:11:57,430 --> 00:11:58,550
The first method that can be

337
00:11:58,550 --> 00:12:00,730
used to train
very large graphs

338
00:12:01,190 --> 00:12:03,015
is based on mini
batch training

339
00:12:03,175 --> 00:12:04,455
using sampling of nodes with

340
00:12:04,455 --> 00:12:05,915
their respective
k hop neighbors.

341
00:12:07,175 --> 00:12:10,295
Of course this method,
for deeper GNN,

342
00:12:10,295 --> 00:12:11,895
for GNN with a large number of

343
00:12:11,895 --> 00:12:13,880
GNN layers, can
face the problem

344
00:12:13,880 --> 00:12:15,000
that we have
mentioned before,

345
00:12:15,480 --> 00:12:17,020
related to
neighborhood explosion

346
00:12:17,880 --> 00:12:18,700
and oversmoothing,

347
00:12:19,880 --> 00:12:22,545
and in general
can cause a large

348
00:12:22,545 --> 00:12:24,305
increase of
computational memory

349
00:12:24,305 --> 00:12:26,865
overhead. This is
the reason why

350
00:12:26,865 --> 00:12:28,785
the earlier
implementation used

351
00:12:28,785 --> 00:12:30,545
the fixed neighborhood
size per layer,

352
00:12:30,545 --> 00:12:32,270
reducing the number
of neighbors

353
00:12:32,270 --> 00:12:33,870
segregated per each message

354
00:12:33,870 --> 00:12:36,110
passing step, which of course

355
00:12:36,110 --> 00:12:38,590
makes for effective
training of

356
00:12:38,590 --> 00:12:39,625
very large graphs,

357
00:12:40,025 --> 00:12:42,345
but has a downside
that corresponds

358
00:12:42,345 --> 00:12:44,285
to a reduction of expressivity

359
00:12:44,425 --> 00:12:46,285
of the model compared
to processing

360
00:12:46,505 --> 00:12:47,870
on the original graph,

361
00:12:48,110 --> 00:12:50,030
which may correspond
to reduced

362
00:12:50,030 --> 00:12:51,650
performance, reduced
task performance.

363
00:12:53,230 --> 00:12:54,590
Alternative
approaches have been

364
00:12:54,590 --> 00:12:56,825
proposed more
recently decouple

365
00:12:57,205 --> 00:12:58,165
the depth of the layer,

366
00:12:58,165 --> 00:12:59,605
the number of message passing

367
00:12:59,605 --> 00:13:01,925
steps with the number of hops

368
00:13:01,925 --> 00:13:03,065
for neighborhood aggregation

369
00:13:03,525 --> 00:13:04,965
with what is referred to as

370
00:13:04,965 --> 00:13:05,945
shallow sampling.

371
00:13:06,710 --> 00:13:09,850
This method can compromise
the computational

372
00:13:09,990 --> 00:13:12,250
efficiency by an aggregating

373
00:13:13,335 --> 00:13:15,255
an excessively high number of

374
00:13:15,255 --> 00:13:17,415
neighbors with an increased

375
00:13:17,415 --> 00:13:19,655
expressivity by
training deeper

376
00:13:19,655 --> 00:13:20,155
networks.

377
00:13:22,970 --> 00:13:24,170
The performance of mini batch

378
00:13:24,170 --> 00:13:26,030
training can be improved by

379
00:13:26,330 --> 00:13:27,950
sampling of connected
subgraphs,

380
00:13:28,090 --> 00:13:29,870
which are provided by
graph clustering.

381
00:13:30,755 --> 00:13:32,215
This method
improves performance

382
00:13:32,595 --> 00:13:34,115
and also increases
the embedding

383
00:13:34,115 --> 00:13:36,455
utilization of
PDMATCH training.

384
00:13:37,235 --> 00:13:38,535
In this case the complexity

385
00:13:38,970 --> 00:13:40,250
increases only
linearly instead

386
00:13:40,250 --> 00:13:41,610
of exponentially
with the number

387
00:13:41,610 --> 00:13:43,930
of GNN layers, which allows to

388
00:13:43,930 --> 00:13:44,970
increase the number of massive

389
00:13:44,970 --> 00:13:47,550
passive steps of having
the deeper GNNs,

390
00:13:48,075 --> 00:13:49,295
which improve the expressivity

391
00:13:49,355 --> 00:13:50,955
of the model and correspond to

392
00:13:50,955 --> 00:13:52,175
improved task performance.

393
00:13:53,355 --> 00:13:54,875
And the performance
can be further

394
00:13:54,875 --> 00:13:56,840
improved by sampling multiple

395
00:13:56,840 --> 00:13:57,740
connected clusters,

396
00:13:58,040 --> 00:13:59,720
which reduces
the variance across

397
00:13:59,720 --> 00:14:00,460
mini batches.

398
00:14:02,040 --> 00:14:03,640
Both mini batch training are

399
00:14:03,640 --> 00:14:06,255
based on selecting nodes with

400
00:14:06,255 --> 00:14:07,555
the corresponding neighborhoods

401
00:14:08,015 --> 00:14:11,535
and selecting
subgraphs provided

402
00:14:11,535 --> 00:14:13,060
by graph clustering can be

403
00:14:13,060 --> 00:14:14,760
efficiently implemented
on the IPU.

404
00:14:15,300 --> 00:14:16,980
Both, in both cases
the mini batch,

405
00:14:16,980 --> 00:14:18,520
the examples of the mini batch

406
00:14:18,580 --> 00:14:20,200
can be stored on
on chip memory,

407
00:14:20,495 --> 00:14:22,255
which provide
efficient and fine

408
00:14:22,255 --> 00:14:23,775
grained execution with fast

409
00:14:23,775 --> 00:14:26,015
access and for neighborhood

410
00:14:26,015 --> 00:14:27,635
aggregation on chip.

411
00:14:32,100 --> 00:14:33,380
The implementation
can be scaled

412
00:14:33,380 --> 00:14:34,820
over a larger
number of processor

413
00:14:34,820 --> 00:14:35,960
by parallel training,

414
00:14:36,695 --> 00:14:38,215
in particular relying on data

415
00:14:38,215 --> 00:14:40,155
parallelism and
pipeline parallelism.

416
00:14:40,935 --> 00:14:42,395
With data parallel training,

417
00:14:42,535 --> 00:14:44,410
the mini batch is distributed

418
00:14:44,710 --> 00:14:47,050
over multiple processors and

419
00:14:47,430 --> 00:14:49,030
using the on chip memory to

420
00:14:49,030 --> 00:14:50,310
store the elements of the mini

421
00:14:50,310 --> 00:14:52,555
batch allows to
continue to have

422
00:14:52,555 --> 00:14:53,435
efficient training,

423
00:14:53,435 --> 00:14:55,135
efficient access to memory,

424
00:14:55,675 --> 00:14:58,075
but increasing the the size of

425
00:14:58,075 --> 00:15:00,095
the mini batch and
speeding up training.

426
00:15:02,460 --> 00:15:04,140
Pipeline parallelism
at the same

427
00:15:04,140 --> 00:15:06,460
time involves
the implementation

428
00:15:06,460 --> 00:15:07,980
of a number of pipeline stages

429
00:15:07,980 --> 00:15:10,145
which can be implemented again

430
00:15:10,285 --> 00:15:12,605
in the on chip
memory of a number

431
00:15:12,605 --> 00:15:15,005
of IPUs. Different pipeline

432
00:15:15,005 --> 00:15:16,925
stages can include
one or multiple

433
00:15:16,925 --> 00:15:18,610
GNN layers, one or multiple

434
00:15:18,610 --> 00:15:19,750
message passing steps,

435
00:15:20,210 --> 00:15:22,210
that corresponds to the same

436
00:15:22,210 --> 00:15:23,650
computation graph
with different

437
00:15:23,650 --> 00:15:25,750
parameters. And
it's particularly

438
00:15:25,810 --> 00:15:28,275
attractive to
consider the storing

439
00:15:28,275 --> 00:15:29,875
the parameters of the message

440
00:15:29,875 --> 00:15:31,955
passing steps on
the in processor

441
00:15:31,955 --> 00:15:33,955
memory on the on chip SRAM to

442
00:15:33,955 --> 00:15:35,415
reduce the amount
of communication

443
00:15:35,795 --> 00:15:36,535
during training.

444
00:15:37,870 --> 00:15:39,390
And both data parallelism and

445
00:15:39,390 --> 00:15:41,230
pipeline parallelism
for speeding

446
00:15:41,230 --> 00:15:43,010
up training can
be complemented

447
00:15:43,150 --> 00:15:44,590
by the use of low precision

448
00:15:44,590 --> 00:15:45,250
number formats,

449
00:15:45,935 --> 00:15:48,835
which allow to store larger

450
00:15:48,975 --> 00:15:51,215
batch sizes and
to be more memory

451
00:15:51,215 --> 00:15:53,370
efficient and to store larger

452
00:15:53,370 --> 00:15:55,710
amount of computation
element,

453
00:15:56,090 --> 00:15:58,250
other larger number of samples

454
00:15:58,250 --> 00:15:59,770
for node wide
sampling or larger

455
00:15:59,770 --> 00:16:01,895
subgraphs in
the case of sampling

456
00:16:01,895 --> 00:16:03,355
of the connected subgraphs.

457
00:16:04,615 --> 00:16:05,895
And the lower precision number

458
00:16:05,895 --> 00:16:10,100
format not only
allow to increase

459
00:16:10,100 --> 00:16:12,200
the the size of the subgraphs

460
00:16:12,340 --> 00:16:13,320
that can be added,

461
00:16:13,700 --> 00:16:16,185
can be stored on on
chip, but also,

462
00:16:16,185 --> 00:16:17,225
as we said before,

463
00:16:17,225 --> 00:16:18,345
corresponds to more efficient

464
00:16:18,345 --> 00:16:19,945
computation and reduced power

465
00:16:19,945 --> 00:16:20,445
consumption.

466
00:16:23,260 --> 00:16:24,700
As an example of efficient

467
00:16:24,700 --> 00:16:26,000
computation on the IPO,

468
00:16:26,540 --> 00:16:28,800
we consider now temporal
graph networks.

469
00:16:29,580 --> 00:16:30,700
This is current work in

470
00:16:30,700 --> 00:16:32,225
collaboration with Michael

471
00:16:32,225 --> 00:16:33,845
Brosteyn and Emmanuel Rossi,

472
00:16:34,545 --> 00:16:36,065
who had developed and proposed

473
00:16:36,065 --> 00:16:37,505
the original temporal graph

474
00:16:37,505 --> 00:16:38,325
network model.

475
00:16:39,590 --> 00:16:41,110
Temporal graph
networks operate

476
00:16:41,110 --> 00:16:43,050
on continuous time
dynamic graph

477
00:16:43,590 --> 00:16:45,290
that are represented as series

478
00:16:45,430 --> 00:16:46,650
of temporal events.

479
00:16:47,425 --> 00:16:48,785
That corresponds
to the evolution

480
00:16:48,785 --> 00:16:51,445
of features or
connections over time.

481
00:16:52,065 --> 00:16:54,165
The temporal event,
as we said before,

482
00:16:54,305 --> 00:16:56,300
for a dynamic graph can be can

483
00:16:56,300 --> 00:16:57,580
be constituted by the addition

484
00:16:57,580 --> 00:17:00,060
or deletion of a note
or the interaction

485
00:17:00,060 --> 00:17:01,280
between pair of notes,

486
00:17:01,740 --> 00:17:03,180
the modification of the edge

487
00:17:03,180 --> 00:17:04,720
between between
different notes.

488
00:17:05,645 --> 00:17:06,845
In the case of the model,

489
00:17:06,845 --> 00:17:08,545
of the temporal graph
network model,

490
00:17:09,485 --> 00:17:11,005
it's the characteristic
is that

491
00:17:11,005 --> 00:17:12,525
the node, each
node is associated

492
00:17:12,525 --> 00:17:14,065
with a memory, a state,

493
00:17:14,380 --> 00:17:16,480
that is updated
after every event.

494
00:17:17,740 --> 00:17:19,100
The computation starts by

495
00:17:19,100 --> 00:17:22,565
collecting a mini
batch of a number

496
00:17:22,565 --> 00:17:24,805
of recent temporal events for

497
00:17:24,805 --> 00:17:27,445
processing and this mini batch

498
00:17:27,445 --> 00:17:29,205
contains the current state of

499
00:17:29,205 --> 00:17:31,210
the nodes involved in the in

500
00:17:31,210 --> 00:17:33,210
the mini batch,

501
00:17:33,210 --> 00:17:35,630
in the events that
we consider.

502
00:17:36,890 --> 00:17:39,545
And this information is then

503
00:17:39,545 --> 00:17:40,905
used to compute a message

504
00:17:40,905 --> 00:17:42,685
associated with the the nodes

505
00:17:43,065 --> 00:17:43,805
under consideration.

506
00:17:44,585 --> 00:17:46,670
And the messages are then used

507
00:17:46,670 --> 00:17:48,450
to update the memory
of the nodes.

508
00:17:49,710 --> 00:17:51,010
The memory update,

509
00:17:51,390 --> 00:17:53,330
it's implemented by
neural network,

510
00:17:54,110 --> 00:17:55,150
typically a GRU,

511
00:17:55,150 --> 00:17:56,415
a Recurrent Neural Network.

512
00:17:57,455 --> 00:17:58,895
And then the updated states,

513
00:17:58,895 --> 00:18:00,435
the data memory of the nodes

514
00:18:00,655 --> 00:18:02,175
together with
the input information

515
00:18:02,175 --> 00:18:03,635
associated with the mini batch

516
00:18:03,990 --> 00:18:05,910
is used for providing node and

517
00:18:05,910 --> 00:18:07,130
updating node embeddings.

518
00:18:08,310 --> 00:18:10,090
Now the node
embedding function

519
00:18:10,150 --> 00:18:12,645
is particularly
critical and is

520
00:18:12,645 --> 00:18:14,425
based on
aggregating information

521
00:18:14,805 --> 00:18:15,945
from the node neighbors.

522
00:18:17,125 --> 00:18:18,645
Because this is justified by

523
00:18:18,645 --> 00:18:20,825
the fact that in these graphs

524
00:18:21,120 --> 00:18:23,380
and some nodes are often

525
00:18:23,680 --> 00:18:24,900
infrequently updated.

526
00:18:25,440 --> 00:18:26,740
Therefore, it is advantageous

527
00:18:27,040 --> 00:18:29,460
to rely on
the connected neighbors

528
00:18:30,185 --> 00:18:32,205
and information to compensate

529
00:18:32,265 --> 00:18:33,225
for the information that is

530
00:18:33,225 --> 00:18:35,385
lacking in up to
date information

531
00:18:35,385 --> 00:18:37,725
that is missing for
specific nodes.

532
00:18:38,480 --> 00:18:40,240
And this is the reason
why the node

533
00:18:40,240 --> 00:18:41,540
embedding
update implementation

534
00:18:42,240 --> 00:18:43,920
relies on, for example,

535
00:18:43,920 --> 00:18:45,540
temporal graph
attention network

536
00:18:45,760 --> 00:18:47,380
or temporal graph
sum networks.

537
00:18:49,325 --> 00:18:51,165
Once the node
embedding has been

538
00:18:51,165 --> 00:18:53,745
updated through this
type of processing,

539
00:18:54,285 --> 00:18:57,270
the updated node
embeddings are fed,

540
00:18:57,750 --> 00:19:00,870
are sent to a decoder which

541
00:19:00,870 --> 00:19:02,870
provides node
classification or

542
00:19:02,870 --> 00:19:04,070
link prediction in the case of

543
00:19:04,070 --> 00:19:05,015
this block diagram.

544
00:19:05,895 --> 00:19:08,055
The task is to
provide prediction

545
00:19:08,055 --> 00:19:09,755
of the future for
future interactions.

546
00:19:11,575 --> 00:19:13,195
This model had been
very successful,

547
00:19:13,650 --> 00:19:15,090
improving by a large amount

548
00:19:15,090 --> 00:19:16,610
the state of
the art in a number

549
00:19:16,610 --> 00:19:18,710
of relevant tasks.

550
00:19:19,810 --> 00:19:21,750
However, as we have
discussed before,

551
00:19:22,505 --> 00:19:24,125
when when considering
dynamic graphs,

552
00:19:24,425 --> 00:19:27,065
they have the issue
that for a large

553
00:19:27,065 --> 00:19:28,905
batch size using
the aggregation

554
00:19:28,905 --> 00:19:31,405
of a large number
of the temporal

555
00:19:31,465 --> 00:19:33,520
events, there is
the possibility

556
00:19:33,740 --> 00:19:35,980
that the later component
of the batch,

557
00:19:35,980 --> 00:19:37,360
the later samples
of the batch,

558
00:19:37,580 --> 00:19:39,100
lack the information
from the area

559
00:19:39,100 --> 00:19:41,385
sample and have the not up to

560
00:19:41,385 --> 00:19:43,085
date information
which can cause

561
00:19:43,385 --> 00:19:45,145
stale update and
degrade the task

562
00:19:45,145 --> 00:19:48,765
performance. We
have implemented

563
00:19:48,825 --> 00:19:51,530
this model on
the IPU and the plot

564
00:19:51,530 --> 00:19:53,370
on the left hand side of this

565
00:19:53,370 --> 00:19:57,150
slide shows in fact
that the average

566
00:19:57,290 --> 00:19:59,390
performance, the average test

567
00:19:59,770 --> 00:20:01,775
precision during training,

568
00:20:03,275 --> 00:20:06,015
it's much degraded for
larger bite size.

569
00:20:07,115 --> 00:20:08,635
During training
increasing the number

570
00:20:08,635 --> 00:20:09,870
of epochs the performance for

571
00:20:09,870 --> 00:20:11,650
larger bite sizes
is significantly

572
00:20:11,950 --> 00:20:13,790
worse than the one for bite

573
00:20:13,790 --> 00:20:15,890
sizes small as
forty or sixty.

574
00:20:16,965 --> 00:20:19,285
However, using batch sizes as

575
00:20:19,285 --> 00:20:21,545
small as forty or
sixty typically

576
00:20:21,845 --> 00:20:23,845
is connected with
a fine grained

577
00:20:23,845 --> 00:20:25,960
computation that is not very

578
00:20:25,960 --> 00:20:27,660
efficient on the conventional

579
00:20:28,120 --> 00:20:28,940
hardware accelerators.

580
00:20:30,040 --> 00:20:31,480
But implementing this type of

581
00:20:31,480 --> 00:20:33,400
computation on
the IPOS is shown

582
00:20:33,400 --> 00:20:34,665
here on the right hand side,

583
00:20:34,665 --> 00:20:36,445
we have been able to maintain

584
00:20:37,305 --> 00:20:38,985
improved task performance at

585
00:20:38,985 --> 00:20:40,665
the same time delivering speed

586
00:20:40,665 --> 00:20:41,485
up of training.

587
00:20:41,950 --> 00:20:43,230
Maintaining a faster training

588
00:20:43,230 --> 00:20:44,430
for the smaller batch size

589
00:20:44,430 --> 00:20:46,430
corresponding to the case of

590
00:20:46,430 --> 00:20:48,770
the large batch
size which doesn't

591
00:20:48,830 --> 00:20:51,410
achieve the same
task performance.

592
00:20:55,615 --> 00:20:57,215
In summary, we have reviewed

593
00:20:57,215 --> 00:20:58,415
the main challenges of graph

594
00:20:58,415 --> 00:21:00,510
neural network processing that

595
00:21:00,510 --> 00:21:01,890
requires fast computation,

596
00:21:02,590 --> 00:21:04,370
a heterogeneous and fragmented

597
00:21:04,430 --> 00:21:06,850
computation with
a high memory load.

598
00:21:08,305 --> 00:21:10,145
And we have discussed
the key features,

599
00:21:10,145 --> 00:21:12,305
the key design
features of the IPU

600
00:21:12,305 --> 00:21:15,630
architecture,
which can rely on

601
00:21:15,630 --> 00:21:17,410
scalable and cost
effective DRAM

602
00:21:17,870 --> 00:21:19,470
and can make use of the BXP

603
00:21:19,470 --> 00:21:21,390
execution and the large amount

604
00:21:21,390 --> 00:21:23,550
of and cheapest RAM to provide

605
00:21:23,550 --> 00:21:25,615
efficient computation for fine

606
00:21:25,615 --> 00:21:27,375
grained elements
associated with

607
00:21:27,375 --> 00:21:29,075
the processing of graph
neural networks.

608
00:21:30,575 --> 00:21:32,115
And we have
considered implementation

609
00:21:32,175 --> 00:21:34,175
opportunities and methods for

610
00:21:34,175 --> 00:21:37,180
training large
graphs on the IPO

611
00:21:37,880 --> 00:21:39,640
and we have given
us a particular

612
00:21:39,640 --> 00:21:41,160
example of efficient
fine grained

613
00:21:41,160 --> 00:21:43,035
computation for a graph neural

614
00:21:43,035 --> 00:21:44,975
network implemented
on the IPU.

615
00:21:47,847 --> 00:21:49,307
And this concludes
the presentation.

